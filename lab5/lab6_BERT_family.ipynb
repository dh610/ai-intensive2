{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dh610/ai-intensive2/blob/main/lab5/lab6_BERT_family.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvCwmW_p1YTg"
      },
      "source": [
        "# Lab 6 : BERT_family"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIt_P5od1YTu"
      },
      "source": [
        "@copyright:\n",
        "    (c) 2023. iKnow Lab. Ajou Univ., All rights reserved.\n",
        "\n",
        "M.S. Student: Wansik-Jo (jws5327@ajou.ac.kr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B5Oc8H31YTx"
      },
      "source": [
        "# For assignment\n",
        "\n",
        "- Python code의 주석 처리되어있는 부분을 구현하면 됩니다.\n",
        "- MD 형식의 Cell의 [BLANK] 부분을 채우면 됩니다.\n",
        "- MD 형식의 Cell의 [ANSWER] 부분 이후에 답을 작성하면 됩니다.\n",
        "- 조교에게 퀴즈의 답과 함께 코드 실행 결과를 보여준 뒤, BB에 제출 후 가시면 됩니다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_7wZUuu1YT0"
      },
      "source": [
        "\n",
        "## 목차\n",
        "\n",
        "1. BERT Fine-tuning with Huggingface\n",
        "    - Dataset\n",
        "    - Tokenizer\n",
        "    - Model\n",
        "    - Trainer\n",
        "    - Training\n",
        "    - Evaluation\n",
        "2. BERT family\n",
        "    - RoBERTa\n",
        "    - DistilBERT\n",
        "    - SpanBERT\n",
        "    - BigBird"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3kTOYd71YT2"
      },
      "source": [
        "## 1. BERT Fine-tuning\n",
        "\n",
        "본 실습에서는 Huggingface의 Transformers 라이브러리를 이용하여 BERT를 fine-tuning하는 과정을 진행한다.\n",
        "\n",
        "해당 과정을 통해 기본적인 Transformers 라이브러리의 사용법을 익히고, Fine-tuning에 대하여 이해한다.\n",
        "\n",
        "또한 해당 과정으로부터 HuggingFace의 다양한 모델들을 사용하는 방법을 익힌다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2YreQeU1YT4"
      },
      "source": [
        "## 1.1 Dataset\n",
        "\n",
        "본 실습에서는 [Yelp Review](https://www.yelp.com/dataset) 데이터셋을 사용한다. 해당 Dataset은 `datasets` 라이브러리를 통해 쉽게 불러올 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlJ37IIY1YT6"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"yelp_review_full\")\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8kw_pDw1YT_"
      },
      "source": [
        "## 1.2 Tokenizer\n",
        "\n",
        "Huggingface의 Transformers 라이브러리에서는 다양한 Tokenizer를 제공한다. 본 실습에서는 BERT를 사용하기 때문에 BERT Tokenizer를 사용한다.\n",
        "\n",
        "AutoTokenizer를 통해 BERT Tokenizer를 불러올 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8fytWuq1YUB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize_function(x):\n",
        "    return tokenizer(x[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsgDJwm01YUC"
      },
      "source": [
        "## 1.3 Model\n",
        "\n",
        "Huggingface의 Transformers 라이브러리에서는 다양한 pre-trained 모델들을 제공한다.\n",
        "\n",
        "또한, 해당 모델들은 다양한 Task에 맞게 구조가 변경되어 있다.\n",
        "\n",
        "본 실습에서는, Sequence Classification Task를 위한 BERT 모델을 불러온다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTMgpaGD1YUD"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkFcvAIb1YUF"
      },
      "outputs": [],
      "source": [
        "print(model.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn0jVTFA1YUG"
      },
      "source": [
        "## 1.4 Trainer\n",
        "\n",
        "Huggingface의 Transformers 라이브러리에서는 Transformer model training에 최적화된 Trainer class를 제공한다.\n",
        "\n",
        "이를 통해 직접 Training loop를 구현하지 않고도 간단하게 Training을 진행할 수 있다.\n",
        "\n",
        "하지만 본 실습에서는, 직접 Training loop를 구현하는 방법을 익히기 위해 Trainer class를 사용하지 않고 진행한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVSbwyL21YUG"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "tokenized_dataset.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCD4sbjD1YUI"
      },
      "outputs": [],
      "source": [
        "small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(5000))\n",
        "small_eval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(5000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lVLEZu91YUI"
      },
      "source": [
        "### Dataloader\n",
        "\n",
        "각 train/test set에 대한 DataLoader를 사용하여 Batch 단위로 데이터를 불러온다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myjIutAF1YUJ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
        "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lhKG24M1YUK"
      },
      "source": [
        "### Optimizer / Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbQeV2lF1YUL"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLOi_q-Q1YUM"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceRV5JHX1YUN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1BZ2r5G1YUN"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        progress_bar.update(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzk5kf4C1YUO"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFABjM3Z1YUP"
      },
      "source": [
        "## 2. BERT family\n",
        "\n",
        "Huggingface의 Transformers 라이브러리에서는 다양한 BERT family 모델들을 제공한다.\n",
        "\n",
        "본 실습에서는 다양한 BERT family 모델들을 사용해보고, 각 모델들의 차이점을 이해한다.\n",
        "\n",
        "[ANSWER]위에서 진행한 Task를 Hugging Face에서 제공하는 다양한 모델에 대해서 진행해보고, 해당 Model의 Configuration을 이해한 뒤, 각 모델들의 차이점을 설명하시오. (마지막 Cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg6KZyvv1YUP"
      },
      "source": [
        "### RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBXp20wq1YUQ"
      },
      "outputs": [],
      "source": [
        "#Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "def tokenize_function(x):\n",
        "    return tokenizer(x[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "#RoBERTa\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=5)\n",
        "\n",
        "print(model.config)\n",
        "\n",
        "#DataLoader\n",
        "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "tokenized_dataset.set_format(\"torch\")\n",
        "\n",
        "small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(5000))\n",
        "small_eval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(5000))\n",
        "\n",
        "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
        "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)\n",
        "\n",
        "##Optimizer and Scheduler\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "#Training\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        progress_bar.update(1)\n",
        "\n",
        "#Evaluation\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "model.eval()\n",
        "\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47_7kadq1YUR"
      },
      "source": [
        "### DistilBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHhH0OjL1YUS"
      },
      "source": [
        "### BigBird"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb8y4RKt1YUS"
      },
      "source": [
        "[ANSWER] : 최소 3개의 Hugging Face BERT Familly Model에 대해 Task를 진행하고, 각 모델의 특장점에 대해 설명하시오."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mILH0K9l1YUT"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cpwan",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}