{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dh610/ai-intensive2/blob/main/lab3/lab3_TF-IDF_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/MyDrive/ai-intensive2"
      ],
      "metadata": {
        "id": "l6RCysvoDvEV",
        "outputId": "f4146fd9-ce08-4a04-934e-3d43b2031924",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'drive/MyDrive/ai-intensive2'\n",
            "/content/drive/MyDrive/ai-intensive2/lab3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jQ3YzzdjEJVx",
        "outputId": "1771a8d0-b7ef-4419-9f77-bf1e737d8acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects:  25% (1/4)\u001b[K\rremote: Compressing objects:  50% (2/4)\u001b[K\rremote: Compressing objects:  75% (3/4)\u001b[K\rremote: Compressing objects: 100% (4/4)\u001b[K\rremote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 3.75 KiB | 7.00 KiB/s, done.\n",
            "From https://github.com/dh610/ai-intensive2\n",
            "   94724d5..aeb3c58  main       -> origin/main\n",
            "Updating 94724d5..aeb3c58\n",
            "Fast-forward\n",
            " lab3/lab3_Word Embedding_assignment.ipynb | 1753 \u001b[32m++++++++++++++++++++++++++\u001b[m\u001b[31m-----------------------\u001b[m\n",
            " 1 file changed, 944 insertions(+), 809 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd lab3"
      ],
      "metadata": {
        "id": "qkA8F3VpEU1B",
        "outputId": "20c1fa67-9334-4410-95fb-ad44122e6013",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'lab3'\n",
            "/content/drive/MyDrive/ai-intensive2/lab3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTOflomPDueU"
      },
      "source": [
        "# Lab 3 : TF-IDF, Cosine Similarity, and Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87mucn4JDueV"
      },
      "source": [
        "@copyright:\n",
        "    (c) 2023. iKnow Lab. Ajou Univ., All rights reserved.\n",
        "\n",
        "M.S. Student: Wansik-Jo (jws5327@ajou.ac.kr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BZsBhWgDueW"
      },
      "source": [
        "# For assignment\n",
        "\n",
        "- Python code의 주석 처리되어있는 부분을 구현하면 됩니다.\n",
        "- MD 형식의 Cell의 [BLANK] 부분을 채우면 됩니다.\n",
        "- MD 형식의 Cell의 [ANSWER] 부분 이후에 답을 작성하면 됩니다.\n",
        "- 조교에게 퀴즈의 답과 함께 코드 실행 결과를 보여준 뒤, BB에 제출 후 가시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0osLgrQDueX"
      },
      "source": [
        "\n",
        "## 목차\n",
        "1. TF-IDF\n",
        "    - TF 계산하기\n",
        "    - IDF 계산하기\n",
        "    - TF-IDF 계산하기\n",
        "2. TF-IDF를 이용한 문서 유사도 계산하기\n",
        "    - 코사인 유사도 계산하기\n",
        "3. TF-IDF를 이용한 문서 분류하기\n",
        "    - 문서 분류하기\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV8D1j5MDueX"
      },
      "source": [
        "## 1. TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzcYJk--DueY"
      },
      "source": [
        "### Document Classification dataset\n",
        "\n",
        "- 이전 실습과 동일한 데이터셋 사용\n",
        "\n",
        "- URL : https://www.kaggle.com/datasets/jensenbaxter/10dataset-text-document-classification\n",
        "\n",
        "- Kaggle 10 group dataset\n",
        "- 각 Document는, 10개의 category 중 하나에 속함\n",
        "- Categories : business, entertainment, food, graphics, historical, medical, politics, space, sport, technologie\n",
        "- URL 또는 강의노트에서 제공되는 data를 받아서 사용"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "P9nLk8ArGTzd",
        "outputId": "557d06a2-4d28-4e01-e9e1-8f59e0c75a6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "htqG1bxoDueZ"
      },
      "outputs": [],
      "source": [
        "#dataset load\n",
        "categories = {'business', 'entertainment', 'food', 'graphics', 'historical', 'medical', 'politics', 'space', 'sport', 'technologie'}\n",
        "\n",
        "import os\n",
        "\n",
        "dataset = []\n",
        "for category in categories:\n",
        "    for filename in os.listdir('data/' + category):\n",
        "        with open('data/' + category + '/' + filename, 'r') as f:\n",
        "            instance = {}\n",
        "            instance['text'] = f.read()\n",
        "            instance['category'] = category\n",
        "            dataset.append(instance)\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(dataset)\n",
        "train_data = dataset[:int(len(dataset) * 0.8)]\n",
        "test_data = dataset[int(len(dataset) * 0.8):]\n",
        "\n",
        "##\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "#You can use pre-defined tools in nltk, like Tokenizer, Stopword list, etc.\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemm = nltk.stem.WordNetLemmatizer()\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "def clean_text(instance):\n",
        "    text = instance['text'].lower()\n",
        "\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    text = nltk.word_tokenize(text)\n",
        "    text = [word for word in text if word not in stopwords]\n",
        "    text = [lemm.lemmatize(word) for word in text]\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "    text = [word for word in text if word not in string.punctuation]\n",
        "\n",
        "    instance['tokens'] = text\n",
        "\n",
        "\n",
        "for instance in train_data:\n",
        "    \"\"\"\n",
        "    Implement text preprocessing here what you think is necessary for the TF-IDF model what ever you want.\n",
        "    You can just use the code from the previous lab.\n",
        "    \"\"\"\n",
        "    clean_text(instance)\n",
        "\n",
        "for instance in test_data:\n",
        "    \"\"\"\n",
        "    Same as here.\n",
        "    \"\"\"\n",
        "    clean_text(instance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi4NkdYkDueb"
      },
      "source": [
        "### TF\n",
        "\n",
        "- 각 document $d$에 대해, 문서 내 각 단어 $t$의 TF(Term Frequency)를 계산한다.\n",
        "\n",
        "- 문서의 단어 수로 나누어 정규화한다.\n",
        "\n",
        "$$ TF(t) = \\frac{\\text{term } t \\text{'s frequency}}{\\text{number of words in document}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "188-89hoDuec",
        "outputId": "9128dda4-4e8d-4edb-dec2-b8f1bdea0c68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'white': 0.006578947368421052, 'admit': 0.006578947368421052, 'balco': 0.006578947368421052, 'drug': 0.006578947368421052, 'link': 0.006578947368421052, 'ban': 0.006578947368421052, 'american': 0.006578947368421052, 'sprinter': 0.006578947368421052, 'kelli': 0.006578947368421052, 'say': 0.006578947368421052, 'knowingli': 0.006578947368421052, 'took': 0.006578947368421052, 'steroid': 0.006578947368421052, 'given': 0.006578947368421052, 'bay': 0.006578947368421052, 'area': 0.006578947368421052, 'lab': 0.006578947368421052, 'cooper': 0.006578947368421052, 'presid': 0.006578947368421052, 'victor': 0.006578947368421052, 'cont': 0.006578947368421052, 'face': 0.006578947368421052, 'feder': 0.006578947368421052, 'trial': 0.006578947368421052, 'next': 0.006578947368421052, 'year': 0.006578947368421052, 'charg': 0.006578947368421052, 'distribut': 0.006578947368421052, 'tax': 0.006578947368421052, 'evas': 0.006578947368421052, 'said': 0.006578947368421052, 'first': 0.006578947368421052, 'tri': 0.006578947368421052, 'cover': 0.006578947368421052, 'he': 0.006578947368421052, 'one': 0.006578947368421052, 'told': 0.006578947368421052, 'wasnt': 0.006578947368421052, 'san': 0.006578947368421052, 'francisco': 0.006578947368421052, 'chronicl': 0.006578947368421052, 'ad': 0.006578947368421052, 'decis': 0.006578947368421052, 'go': 0.006578947368421052, 'anybodi': 0.006578947368421052, 'el': 0.006578947368421052, 'substanc': 0.006578947368421052, 'flaxse': 0.006578947368421052, 'oil': 0.006578947368421052, 'chang': 0.006578947368421052, 'stori': 0.006578947368421052, 'later': 0.006578947368421052, 'fail': 0.006578947368421052, 'test': 0.006578947368421052, 'win': 0.006578947368421052, 'titl': 0.006578947368421052, 'world': 0.006578947368421052, 'athlet': 0.006578947368421052, 'championship': 0.006578947368421052, 'subsequ': 0.006578947368421052, 'hand': 0.006578947368421052, 'twoyear': 0.006578947368421052, 'may': 0.006578947368421052, 'take': 0.006578947368421052, 'stimul': 0.006578947368421052, 'modafinil': 0.006578947368421052, 'claim': 0.006578947368421052, 'combat': 0.006578947368421052, 'narcolepsi': 0.006578947368421052, 'full': 0.006578947368421052, 'respons': 0.006578947368421052, 'action': 0.006578947368421052, 'whole': 0.006578947368421052, 'belief': 0.006578947368421052, 'sell': 0.006578947368421052, 'product': 0.006578947368421052, 'la': 0.006578947368421052, 'time': 0.006578947368421052, 'whether': 0.006578947368421052, 'good': 0.006578947368421052, 'bad': 0.006578947368421052, 'introduc': 0.006578947368421052, 'coach': 0.006578947368421052, 'remi': 0.006578947368421052, 'korchemi': 0.006578947368421052, 'also': 0.006578947368421052, 'defend': 0.006578947368421052, 'case': 0.006578947368421052, 'yearold': 0.006578947368421052, 'dope': 0.006578947368421052, 'common': 0.006578947368421052, 'sport': 0.006578947368421052, 'felt': 0.006578947368421052, 'compel': 0.006578947368421052, 'cheat': 0.006578947368421052, 'chanc': 0.006578947368421052, 'clue': 0.006578947368421052, 'would': 0.006578947368421052, 'made': 0.006578947368421052, 'mistak': 0.006578947368421052, 'never': 0.006578947368421052, 'ever': 0.006578947368421052, 'back': 0.006578947368421052, 'recommend': 0.006578947368421052, 'anyon': 0.006578947368421052, 'rout': 0.006578947368421052}\n"
          ]
        }
      ],
      "source": [
        "def compute_tf(tokens):\n",
        "    tf_dict = {}\n",
        "\n",
        "    \"\"\"\n",
        "    You need to implement the computation of the term frequency here.\n",
        "    # tf_dict = {'word1': 0, 'word2': 0, ...}\n",
        "    for word in tokens:\n",
        "        tf_dict[word] =\n",
        "\n",
        "    # normalize the term frequency\n",
        "    for word, count in tf_dict.items():\n",
        "        tf_dict[word] =\n",
        "\n",
        "    \"\"\"\n",
        "    for word in tokens:\n",
        "        if word not in tf_dict:\n",
        "           tf_dict[word] = tf_dict.get(word, 0) + 1\n",
        "\n",
        "    for word, count in tf_dict.items():\n",
        "        tf_dict[word] = count / float(len(tokens))\n",
        "\n",
        "    return tf_dict\n",
        "\n",
        "for instance in train_data:\n",
        "    instance['tf'] = compute_tf(instance['tokens'])\n",
        "\n",
        "for instance in test_data:\n",
        "    instance['tf'] = compute_tf(instance['tokens'])\n",
        "\n",
        "print(train_data[0]['tf'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAlFwM0eDuec"
      },
      "source": [
        "### IDF\n",
        "\n",
        "- 각 단어 $t$에 대해, IDF(Inverse Document Frequency)를 계산한다.\n",
        "\n",
        "$$ IDF(t) = log(\\frac{\\text{total number of document}}{\\text{number of document which contains } t}) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DdQRD-54Dued",
        "outputId": "85c0c7d3-7ab0-4686-f3ef-f2d7b018be6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.855970331178832\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def compute_idf(documents):\n",
        "    idf_dict = {}\n",
        "    N = len(documents)\n",
        "\n",
        "    \"\"\"\n",
        "    you need to implement the computation of the inverse document frequency here.\n",
        "    # idf_dict = {'word1': 0, 'word2': 0, ...}\n",
        "    for document in documents:\n",
        "        for word in set(document['tokens']):\n",
        "            idf_dict[word] =\n",
        "\n",
        "    # log normalization\n",
        "    for word, count in idf_dict.items():\n",
        "        idf_dict[word] =\n",
        "    \"\"\"\n",
        "\n",
        "    for document in documents:\n",
        "        for word in set(document['tokens']):\n",
        "            idf_dict[word] = idf_dict.get(word, 0) + 1\n",
        "\n",
        "    for word, count in idf_dict.items():\n",
        "        idf_dict[word] = math.log(N / float(count))\n",
        "\n",
        "    return idf_dict\n",
        "\n",
        "idf = compute_idf(train_data)\n",
        "\n",
        "print(idf['music'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Z7secJDued"
      },
      "source": [
        "### TF-IDF\n",
        "\n",
        "- 각 단어 $t$에 대해, TF-IDF를 다음과 같이 계산한다.\n",
        "\n",
        "$$ TF-IDF(t) = TF(t) \\times IDF(t) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3-02SHdIDued",
        "outputId": "9b6810ed-ea6e-4608-9ffc-d8e94a570c2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'white': 0.02117681463729079, 'admit': 0.02077796844113004, 'balco': 0.030297172276237443, 'drug': 0.01893387656511584, 'link': 0.021601410171090605, 'ban': 0.02182444669527272, 'american': 0.015841747425341262, 'sprinter': 0.02280089409736662, 'kelli': 0.023069459850789353, 'say': 0.009120357638946648, 'knowingli': 0.03338930141601201, 'took': 0.015066859032865055, 'steroid': 0.02820208193993129, 'given': 0.014301001454941297, 'bay': 0.02496210506428791, 'area': 0.015315150664331156, 'lab': 0.024268943777065368, 'cooper': 0.02460639966119399, 'presid': 0.017264267875799395, 'victor': 0.02616158899056393, 'cont': 0.03338930141601201, 'face': 0.014017992342548594, 'feder': 0.019081724300984645, 'trial': 0.018240715277893296, 'next': 0.01050668021339173, 'year': 0.005108742036177607, 'charg': 0.0178573671981303, 'distribut': 0.020401926244814586, 'tax': 0.020974369514483202, 'evas': 0.03485735109571076, 'said': 0.004984774353273135, 'first': 0.007573830519344783, 'tri': 0.012056456998344145, 'cover': 0.016217789621656712, 'he': 0.027629638670262677, 'one': 0.005326848663262479, 'told': 0.012160064609976639, 'wasnt': 0.030297172276237443, 'san': 0.021601410171090605, 'francisco': 0.02710304190925257, 'chronicl': 0.036749996309209325, 'ad': 0.01075497184485783, 'decis': 0.014521545481511322, 'go': 0.009740822635467973, 'anybodi': 0.025338147260603364, 'el': 0.02616158899056393, 'substanc': 0.02710304190925257, 'flaxse': 0.03941752991518409, 'oil': 0.0178573671981303, 'chang': 0.012591625678962251, 'stori': 0.019875329483804476, 'later': 0.013359548536252657, 'fail': 0.016825508833044973, 'test': 0.014447206302209126, 'win': 0.013949102211842703, 'titl': 0.0178573671981303, 'world': 0.008404210238209265, 'athlet': 0.018373627826034872, 'championship': 0.02117681463729079, 'subsequ': 0.024268943777065368, 'hand': 0.014672792311936972, 'twoyear': 0.028829122596538692, 'may': 0.00928692216515908, 'take': 0.008894032463174724, 'stimul': 0.030297172276237443, 'modafinil': 0.03941752991518409, 'claim': 0.014017992342548594, 'combat': 0.02460639966119399, 'narcolepsi': 0.043977708734657416, 'full': 0.01811043483515527, 'respons': 0.015066859032865055, 'action': 0.01566149101620893, 'whole': 0.01651463459060717, 'belief': 0.01938777703542473, 'sell': 0.022294571458313147, 'product': 0.013297188378656974, 'la': 0.023349457314070902, 'time': 0.0068832701206196145, 'whether': 0.015841747425341262, 'good': 0.01180435920688272, 'bad': 0.01893387656511584, 'introduc': 0.02077796844113004, 'coach': 0.024268943777065368, 'remi': 0.036749996309209325, 'korchemi': 0.043977708734657416, 'also': 0.0046429339655710885, 'defend': 0.01954631355897118, 'case': 0.013486070654199286, 'yearold': 0.017734392638839823, 'dope': 0.02661548946087282, 'common': 0.017378687235745902, 'sport': 0.019708764957592044, 'felt': 0.023069459850789353, 'compel': 0.02820208193993129, 'cheat': 0.036749996309209325, 'chanc': 0.017734392638839823, 'clue': 0.032189817489736004, 'would': 0.0057268499726503995, 'made': 0.009704773518595436, 'mistak': 0.029522283883761234, 'never': 0.014986134739497855, 'ever': 0.0178573671981303, 'back': 0.010797280333872897, 'recommend': 0.021601410171090605, 'anyon': 0.016315257291295796, 'rout': 0.022294571458313147}\n",
            "{'lawyer': 0.013046915017722236, 'attack': 0.0081644891770741, 'antiterror': 0.018190587151375722, 'law': 0.009797017953715053, 'senior': 0.011890021138643202, 'barrist': 0.020021503365590745, 'resign': 0.014500194975099173, 'protest': 0.013221790158114467, 'govern': 0.006072326669853731, 'say': 0.004968796993261256, 'current': 0.007144314858203525, 'system': 0.006983560140764989, 'give': 0.007015139458622175, 'britain': 0.007637042423180597, 'bad': 0.0103152302433606, 'name': 0.0081644891770741, 'ian': 0.016505986329706422, 'macdonald': 0.021474783322967678, 'qc': 0.023959181819598305, 'quit': 0.009728744853461669, 'fail': 0.00916658545742952, 'recognis': 0.013046915017722236, 'hous': 0.009797017953715053, 'lord': 0.011650983954060147, 'rule': 0.00799377932406602, 'detain': 0.018190587151375722, 'terror': 0.013804295281762406, 'suspect': 0.013405637091403178, 'indefinit': 0.018190587151375722, 'unlaw': 0.023959181819598305, 'part': 0.005724069506937431, 'strong': 0.009344352271549132, 'panel': 0.013046915017722236, 'special': 0.008997220278753729, 'secur': 0.008208468003568059, 'clearanc': 0.016984593471729797, 'act': 0.00959598043883676, 'terrorist': 0.015364575107059342, 'five': 0.00799377932406602, 'report': 0.006351099791870521, 'threaten': 0.01256830787569886, 'mr': 0.006048026716501119, 'told': 0.006624838067084047, 'bbc': 0.0081644891770741, 'news': 0.007453195489891884, 'reason': 0.008630629421691297, 'fundament': 0.014765814946976312, 'disagre': 0.017537104868960117, 'lock': 0.020021503365590745, 'peopl': 0.0042707895495604975, 'without': 0.0077133055000525854, 'trial': 0.009937593986522513, 'period': 0.009224866894245936, 'suspicion': 0.016083824911583184, 'legal': 0.011016823709762376, 'certainli': 0.011890021138643202, 'advers': 0.017537104868960117, 'effect': 0.007312619457084425, 'muslim': 0.014252908697368163, 'commun': 0.008208468003568059, 'whole': 0.008997220278753729, 'asian': 0.014252908697368163, 'think': 0.006458099659790525, 'intern': 0.00748921819904422, 'crime': 0.013405637091403178, 'introduc': 0.011319896785131778, 'respons': 0.008208468003568059, 'septemb': 0.00959598043883676, 'foreign': 0.010395778113798087, 'nation': 0.00665339781493056, 'involv': 0.008208468003568059, 'deport': 0.018990384826337046, 'held': 0.009109236563714675, 'belief': 0.01056251652109161, 'detaine': 0.018990384826337046, 'entitl': 0.015706188654745094, 'juri': 0.016984593471729797, 'view': 0.009405622642012574, 'need': 0.0054474041352846345, 'full': 0.009866616827754844, 'return': 0.0077133055000525854, 'proper': 0.014500194975099173, 'crimin': 0.011890021138643202, 'accus': 0.011215998803507217, 'far': 0.007791226599107804, 'im': 0.008389986759190837, 'concern': 0.008680759321242336, 'start': 0.006404200457767083, 'rethink': 0.018990384826337046, 'strategi': 0.01092123859477255, 'deal': 0.009224866894245936, 'ad': 0.005859339499707492, 'attorney': 0.016984593471729797, 'gener': 0.005746262948686251, 'goldsmith': 0.021474783322967678, 'receiv': 0.008343737996338122, 'letter': 0.013046915017722236, 'monday': 0.011016823709762376, 'accord': 0.007525606627230664, 'independ': 0.009405622642012574, 'expect': 0.006377551789353038, 'follow': 0.006072326669853731, 'nichola': 0.016083824911583184, 'blake': 0.023959181819598305, 'andrew': 0.014252908697368163, 'nicol': 0.023959181819598305, 'manjit': 0.023959181819598305, 'singh': 0.020021503365590745, 'gill': 0.018990384826337046, 'rick': 0.018990384826337046, 'scannel': 0.023959181819598305, 'tom': 0.014765814946976312, 'de': 0.011319896785131778, 'la': 0.012720851296554758, 'mare': 0.023959181819598305, 'believ': 0.0074175312216009604, 'care': 0.009866616827754844, 'consid': 0.009052790839814535, 'posit': 0.00812104344039903, 'advoc': 0.014500194975099173, 'repres': 0.009661747960944994, 'immigr': 0.013804295281762406, 'appeal': 0.011115027918321925, 'commiss': 0.011319896785131778, 'siac': 0.023959181819598305, 'court': 0.011016823709762376, 'tri': 0.006568392343183908, 'said': 0.002715719360922998, 'idea': 0.008630629421691297, 'whether': 0.008630629421691297, 'would': 0.0031200042861751283, 'barri': 0.016083824911583184, 'hugil': 0.023959181819598305, 'spokesman': 0.009797017953715053, 'campaign': 0.008942498156421154, 'group': 0.007347249962144415, 'liberti': 0.015052706372329487, 'radio': 0.010395778113798087, 'today': 0.00916658545742952, 'programm': 0.009728744853461669, 'may': 0.005059541824746166, 'go': 0.005306828102477176, 'assur': 0.015052706372329487, 'distinct': 0.016505986329706422, 'possibilti': 0.023959181819598305, 'situat': 0.010828136492968748, 'everyth': 0.011319896785131778, 'train': 0.009866616827754844, 'right': 0.007111581942206132, 'abandon': 0.013804295281762406, 'sleepless': 0.023959181819598305, 'night': 0.011426896653051782, 'helena': 0.023959181819598305, 'kennedi': 0.013221790158114467, 'labour': 0.010010005123861293, 'peer': 0.015052706372329487, 'human': 0.009728744853461669, 'main': 0.008436840098141923, 'seen': 0.008835498288501149, 'intellig': 0.014765814946976312, 'allow': 0.007312619457084425, 'speak': 0.010159369627089136, 'procedur': 0.013405637091403178, 'immedi': 0.01092123859477255, 'great': 0.006983560140764989, 'argument': 0.013599426414952554, 'particularli': 0.00959598043883676, 'realli': 0.008630629421691297, 'process': 0.00799377932406602, 'could': 0.004330280199772102, 'judici': 0.018190587151375722, 'review': 0.01064888767370473, 'detent': 0.018990384826337046, 'habeu': 0.023959181819598305, 'corpu': 0.018990384826337046, 'blot': 0.023959181819598305, 'mcdonald': 0.018190587151375722, 'landscap': 0.015706188654745094, 'someth': 0.008680759321242336, 'shock': 0.013804295281762406, 'regard': 0.011537189336445163}\n"
          ]
        }
      ],
      "source": [
        "def compute_tfidf(tf, idf):\n",
        "    tfidf = {word: tf[word] * idf.get(word, 0) for word in tf}\n",
        "    return tfidf\n",
        "\n",
        "# Compute TF-IDF for training data\n",
        "for instance in train_data:\n",
        "    instance['tfidf'] = compute_tfidf(instance['tf'], idf)\n",
        "\n",
        "for instance in test_data:\n",
        "    instance['tfidf'] = compute_tfidf(instance['tf'], idf)\n",
        "\n",
        "print(train_data[0]['tfidf'])\n",
        "print(train_data[1]['tfidf'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQkuhlX_Duee"
      },
      "source": [
        "내 모든 문서 $d$에 대해, 각 단어 $t$의 TF-IDF matrix를 얻을 수 있게 되었다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbBWc5V4Duee"
      },
      "source": [
        "## 2. TF-IDF를 이용한 문서 유사도 계산하기\n",
        "\n",
        "\n",
        "해당 TF-IDF matrix를 이용하여, 문서 간 유사도를 계산할 수 있다.\n",
        "\n",
        "만약 문서 $d_1$과 문서 $d_2$가 주어졌을 때, 문서 간 유사도를 계산하는 방법은 다음과 같다.\n",
        "\n",
        "1. 문서 $d_1$과 문서 $d_2$에 대한 TF-IDF matrix를 얻는다.\n",
        "2. 문서 $d_1$과 문서 $d_2$의 TF-IDF matrix를 이용하여, 코사인 유사도를 계산한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWOJd6U-Duee"
      },
      "source": [
        "### Cosine Similarity\n",
        "\n",
        "- 두 벡터 $v_1$과 $v_2$에 대해, 코사인 유사도는 다음과 같이 계산한다.\n",
        "\n",
        "$$ \\text{Cosine Similarity} = \\frac{v_1 \\cdot v_2}{\\text{norm of } v_1 \\times \\text{norm of } v_2} $$\n",
        "\n",
        "- Document Classification task에서는, 문서 $d_1$과 문서 $d_2$의 TF-IDF matrix를 이용하여, 코사인 유사도를 계산한다.\n",
        "\n",
        "$$ \\text{Cosine Similarity} = \\frac{\\text{TF-IDF vector 1} \\cdot \\text{TF-IDF vector 2}}{\\text{norm of TF-IDF vector 1} \\times \\text{norm of TF-IDF vector 2}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bHwCgE4eDuee"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    # To do deal with the words that are not in the vocabulary you need to check if the word is in the vocabulary.\n",
        "    dot_product = sum([vec1[word] * vec2[word] for word in vec1 if word in vec2])\n",
        "\n",
        "    \"\"\"\n",
        "    you need to implement the computation of the magnitude of the vectors here.\n",
        "\n",
        "    vec1_norm =\n",
        "    vec2_norm =\n",
        "    \"\"\"\n",
        "    vec1_norm = math.sqrt(sum([value ** 2 for value in vec1.values()]))\n",
        "    vec2_norm = math.sqrt(sum([value ** 2 for value in vec2.values()]))\n",
        "\n",
        "    if not vec1_norm or not vec2_norm: # if one of the vectors is empty return 0\n",
        "        return 0\n",
        "\n",
        "    cosine_sim = dot_product / (vec1_norm * vec2_norm)\n",
        "\n",
        "    return cosine_sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPxowgOCDuee"
      },
      "source": [
        "## 3. TF-IDF를 이용한 문서 분류하기\n",
        "\n",
        "TF-IDF와 Cosine Similarity를 이용하여 classification task를 다음과 같이 진행할 수 있다.\n",
        "\n",
        "1. Training set의 각 category의 문서들에 대해, TF-IDF를 구해 평균을 계산한다.\n",
        "2. Inference시, test set의 instance에 대해, TF-IDF를 구한다.\n",
        "3. Training set의 각 category의 평균 TF-IDF와 test set의 instance의 TF-IDF를 이용하여, 코사인 유사도를 계산한다.\n",
        "4. 가장 높은 코사인 유사도를 가진 category로 분류한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ldp0sN1MDuef",
        "outputId": "30b40483-ed25-4348-f635-2b8e97b4d93c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.370942213326038e-07\n",
            "5.2999650575329006e-05\n"
          ]
        }
      ],
      "source": [
        "def compute_class_avg_tfidf(data):\n",
        "    class_avg_tfidf = {}\n",
        "    class_counts = {}\n",
        "\n",
        "    \"\"\"\n",
        "    you need to implement the computation of the class average TF-IDF here.\n",
        "    for instance in data:\n",
        "        category =\n",
        "        tfidf =\n",
        "\n",
        "        if category not in class_avg_tfidf:\n",
        "            ~~\n",
        "        else :\n",
        "            ~~\n",
        "\n",
        "    for category in class_avg_tfidf:\n",
        "        for word in class_avg_tfidf[category]:\n",
        "            # normalize the class average TF-IDF\n",
        "\n",
        "    \"\"\"\n",
        "    for instance in data:\n",
        "        category = instance['category']\n",
        "        tfidf = instance['tfidf']\n",
        "\n",
        "        if category not in class_avg_tfidf:\n",
        "            class_avg_tfidf[category] = tfidf\n",
        "            class_counts[category] = 1\n",
        "        else:\n",
        "            for word in tfidf:\n",
        "                if word not in class_avg_tfidf[category]:\n",
        "                    class_avg_tfidf[category][word] = tfidf[word]\n",
        "                else:\n",
        "                    class_avg_tfidf[category][word] += tfidf[word]\n",
        "\n",
        "                class_counts[category] += 1\n",
        "\n",
        "    for category in class_avg_tfidf:\n",
        "        for word in class_avg_tfidf[category]:\n",
        "            class_avg_tfidf[category][word] /= class_counts[category]\n",
        "\n",
        "\n",
        "    return class_avg_tfidf\n",
        "\n",
        "class_avg_tfidf = compute_class_avg_tfidf(train_data)\n",
        "\n",
        "print(class_avg_tfidf['business']['music'])\n",
        "print(class_avg_tfidf['entertainment']['music'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9WY2CwgRDuef",
        "outputId": "b717586c-126a-4498-a549-be019bc35a73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.945\n"
          ]
        }
      ],
      "source": [
        "def classify(instance, class_avg_tfidf):\n",
        "    instance_tfidf = instance['tfidf']\n",
        "    category_scores = {category: cosine_similarity(instance_tfidf, class_avg_tfidf[category]) for category in class_avg_tfidf}\n",
        "    return max(category_scores, key=category_scores.get)\n",
        "\n",
        "def evaluate(test_data, class_avg_tfidf):\n",
        "    correct = 0\n",
        "    for instance in test_data:\n",
        "        predicted = classify(instance, class_avg_tfidf)\n",
        "        if predicted == instance['category']:\n",
        "            correct += 1\n",
        "    return correct / float(len(test_data))\n",
        "\n",
        "print(evaluate(test_data, class_avg_tfidf))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cpwan",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}