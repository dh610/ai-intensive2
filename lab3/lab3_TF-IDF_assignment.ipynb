{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dh610/ai-intensive2/blob/main/lab3/lab3_TF-IDF_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/MyDrive/ai-intensive2"
      ],
      "metadata": {
        "id": "l6RCysvoDvEV",
        "outputId": "4dee2796-4aed-4b17-fcbf-0425f0ed07bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ai-intensive2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jQ3YzzdjEJVx",
        "outputId": "85f1af6c-0c53-470a-8d8b-68a9a5e73fc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/14)\u001b[K\rremote: Counting objects:  14% (2/14)\u001b[K\rremote: Counting objects:  21% (3/14)\u001b[K\rremote: Counting objects:  28% (4/14)\u001b[K\rremote: Counting objects:  35% (5/14)\u001b[K\rremote: Counting objects:  42% (6/14)\u001b[K\rremote: Counting objects:  50% (7/14)\u001b[K\rremote: Counting objects:  57% (8/14)\u001b[K\rremote: Counting objects:  64% (9/14)\u001b[K\rremote: Counting objects:  71% (10/14)\u001b[K\rremote: Counting objects:  78% (11/14)\u001b[K\rremote: Counting objects:  85% (12/14)\u001b[K\rremote: Counting objects:  92% (13/14)\u001b[K\rremote: Counting objects: 100% (14/14)\u001b[K\rremote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects:  10% (1/10)\u001b[K\rremote: Compressing objects:  20% (2/10)\u001b[K\rremote: Compressing objects:  30% (3/10)\u001b[K\rremote: Compressing objects:  40% (4/10)\u001b[K\rremote: Compressing objects:  50% (5/10)\u001b[K\rremote: Compressing objects:  60% (6/10)\u001b[K\rremote: Compressing objects:  70% (7/10)\u001b[K\rremote: Compressing objects:  80% (8/10)\u001b[K\rremote: Compressing objects:  90% (9/10)\u001b[K\rremote: Compressing objects: 100% (10/10)\u001b[K\rremote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 12 (delta 4), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (12/12), 12.60 KiB | 16.00 KiB/s, done.\n",
            "From https://github.com/dh610/ai-intensive2\n",
            "   76d1ed6..7a560a6  main       -> origin/main\n",
            "Updating 76d1ed6..7a560a6\n",
            "Fast-forward\n",
            " lab2_naive_bayes_assignment.ipynb         |  46 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n",
            " lab3/lab3.md                              |   1 \u001b[32m+\u001b[m\n",
            " lab3/lab3_TF-IDF_assignment.ipynb         | 389 \u001b[32m++++++++++++++++++++++++\u001b[m\n",
            " lab3/lab3_Word Embedding_assignment.ipynb | 810 \u001b[32m++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " 4 files changed, 1230 insertions(+), 16 deletions(-)\n",
            " create mode 100644 lab3/lab3.md\n",
            " create mode 100644 lab3/lab3_TF-IDF_assignment.ipynb\n",
            " create mode 100644 lab3/lab3_Word Embedding_assignment.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd lab3"
      ],
      "metadata": {
        "id": "qkA8F3VpEU1B",
        "outputId": "c49704af-6425-47f6-e3fa-3084aede107c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ai-intensive2/lab3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTOflomPDueU"
      },
      "source": [
        "# Lab 3 : TF-IDF, Cosine Similarity, and Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87mucn4JDueV"
      },
      "source": [
        "@copyright:\n",
        "    (c) 2023. iKnow Lab. Ajou Univ., All rights reserved.\n",
        "\n",
        "M.S. Student: Wansik-Jo (jws5327@ajou.ac.kr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BZsBhWgDueW"
      },
      "source": [
        "# For assignment\n",
        "\n",
        "- Python code의 주석 처리되어있는 부분을 구현하면 됩니다.\n",
        "- MD 형식의 Cell의 [BLANK] 부분을 채우면 됩니다.\n",
        "- MD 형식의 Cell의 [ANSWER] 부분 이후에 답을 작성하면 됩니다.\n",
        "- 조교에게 퀴즈의 답과 함께 코드 실행 결과를 보여준 뒤, BB에 제출 후 가시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0osLgrQDueX"
      },
      "source": [
        "\n",
        "## 목차\n",
        "1. TF-IDF\n",
        "    - TF 계산하기\n",
        "    - IDF 계산하기\n",
        "    - TF-IDF 계산하기\n",
        "2. TF-IDF를 이용한 문서 유사도 계산하기\n",
        "    - 코사인 유사도 계산하기\n",
        "3. TF-IDF를 이용한 문서 분류하기\n",
        "    - 문서 분류하기\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV8D1j5MDueX"
      },
      "source": [
        "## 1. TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzcYJk--DueY"
      },
      "source": [
        "### Document Classification dataset\n",
        "\n",
        "- 이전 실습과 동일한 데이터셋 사용\n",
        "\n",
        "- URL : https://www.kaggle.com/datasets/jensenbaxter/10dataset-text-document-classification\n",
        "\n",
        "- Kaggle 10 group dataset\n",
        "- 각 Document는, 10개의 category 중 하나에 속함\n",
        "- Categories : business, entertainment, food, graphics, historical, medical, politics, space, sport, technologie\n",
        "- URL 또는 강의노트에서 제공되는 data를 받아서 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htqG1bxoDueZ"
      },
      "outputs": [],
      "source": [
        "#dataset load\n",
        "categories = {'business', 'entertainment', 'food', 'graphics', 'historical', 'medical', 'politics', 'space', 'sport', 'technologie'}\n",
        "\n",
        "import os\n",
        "\n",
        "dataset = []\n",
        "for category in categories:\n",
        "    for filename in os.listdir('data/' + category):\n",
        "        with open('data/' + category + '/' + filename, 'r') as f:\n",
        "            instance = {}\n",
        "            instance['text'] = f.read()\n",
        "            instance['category'] = category\n",
        "            dataset.append(instance)\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(dataset)\n",
        "train_data = dataset[:int(len(dataset) * 0.8)]\n",
        "test_data = dataset[int(len(dataset) * 0.8):]\n",
        "\n",
        "##\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "for instance in train_data:\n",
        "    \"\"\"\n",
        "    Implement text preprocessing here what you think is necessary for the TF-IDF model what ever you want.\n",
        "    You can just use the code from the previous lab.\n",
        "    \"\"\"\n",
        "\n",
        "for instance in test_data:\n",
        "    \"\"\"\n",
        "    Same as here.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi4NkdYkDueb"
      },
      "source": [
        "### TF\n",
        "\n",
        "- 각 document $d$에 대해, 문서 내 각 단어 $t$의 TF(Term Frequency)를 계산한다.\n",
        "\n",
        "- 문서의 단어 수로 나누어 정규화한다.\n",
        "\n",
        "$$ TF(t) = \\frac{\\text{term } t \\text{'s frequency}}{\\text{number of words in document}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "188-89hoDuec"
      },
      "outputs": [],
      "source": [
        "def compute_tf(tokens):\n",
        "    tf_dict = {}\n",
        "\n",
        "    \"\"\"\n",
        "    You need to implement the computation of the term frequency here.\n",
        "    # tf_dict = {'word1': 0, 'word2': 0, ...}\n",
        "    for word in tokens:\n",
        "        tf_dict[word] =\n",
        "\n",
        "    # normalize the term frequency\n",
        "    for word, count in tf_dict.items():\n",
        "        tf_dict[word] =\n",
        "\n",
        "    \"\"\"\n",
        "    return tf_dict\n",
        "\n",
        "for instance in train_data:\n",
        "    instance['tf'] = compute_tf(instance['tokens'])\n",
        "\n",
        "for instance in test_data:\n",
        "    instance['tf'] = compute_tf(instance['tokens'])\n",
        "\n",
        "print(train_data[0]['tf'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAlFwM0eDuec"
      },
      "source": [
        "### IDF\n",
        "\n",
        "- 각 단어 $t$에 대해, IDF(Inverse Document Frequency)를 계산한다.\n",
        "\n",
        "$$ IDF(t) = log(\\frac{\\text{total number of document}}{\\text{number of document which contains } t}) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdQRD-54Dued"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def compute_idf(documents):\n",
        "    idf_dict = {}\n",
        "    N = len(documents)\n",
        "\n",
        "    \"\"\"\n",
        "    you need to implement the computation of the inverse document frequency here.\n",
        "    # idf_dict = {'word1': 0, 'word2': 0, ...}\n",
        "    for document in documents:\n",
        "        for word in set(document['tokens']):\n",
        "            idf_dict[word] =\n",
        "\n",
        "    # log normalization\n",
        "    for word, count in idf_dict.items():\n",
        "        idf_dict[word] =\n",
        "    \"\"\"\n",
        "\n",
        "    return idf_dict\n",
        "\n",
        "idf = compute_idf(train_data)\n",
        "\n",
        "print(idf['music'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Z7secJDued"
      },
      "source": [
        "### TF-IDF\n",
        "\n",
        "- 각 단어 $t$에 대해, TF-IDF를 다음과 같이 계산한다.\n",
        "\n",
        "$$ TF-IDF(t) = TF(t) \\times IDF(t) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-02SHdIDued"
      },
      "outputs": [],
      "source": [
        "def compute_tfidf(tf, idf):\n",
        "    tfidf = {word: tf[word] * idf.get(word, 0) for word in tf}\n",
        "    return tfidf\n",
        "\n",
        "# Compute TF-IDF for training data\n",
        "for instance in train_data:\n",
        "    instance['tfidf'] = compute_tfidf(instance['tf'], idf)\n",
        "\n",
        "for instance in test_data:\n",
        "    instance['tfidf'] = compute_tfidf(instance['tf'], idf)\n",
        "\n",
        "print(train_data[0]['tfidf'])\n",
        "print(train_data[1]['tfidf'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQkuhlX_Duee"
      },
      "source": [
        "내 모든 문서 $d$에 대해, 각 단어 $t$의 TF-IDF matrix를 얻을 수 있게 되었다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbBWc5V4Duee"
      },
      "source": [
        "## 2. TF-IDF를 이용한 문서 유사도 계산하기\n",
        "\n",
        "\n",
        "해당 TF-IDF matrix를 이용하여, 문서 간 유사도를 계산할 수 있다.\n",
        "\n",
        "만약 문서 $d_1$과 문서 $d_2$가 주어졌을 때, 문서 간 유사도를 계산하는 방법은 다음과 같다.\n",
        "\n",
        "1. 문서 $d_1$과 문서 $d_2$에 대한 TF-IDF matrix를 얻는다.\n",
        "2. 문서 $d_1$과 문서 $d_2$의 TF-IDF matrix를 이용하여, 코사인 유사도를 계산한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWOJd6U-Duee"
      },
      "source": [
        "### Cosine Similarity\n",
        "\n",
        "- 두 벡터 $v_1$과 $v_2$에 대해, 코사인 유사도는 다음과 같이 계산한다.\n",
        "\n",
        "$$ \\text{Cosine Similarity} = \\frac{v_1 \\cdot v_2}{\\text{norm of } v_1 \\times \\text{norm of } v_2} $$\n",
        "\n",
        "- Document Classification task에서는, 문서 $d_1$과 문서 $d_2$의 TF-IDF matrix를 이용하여, 코사인 유사도를 계산한다.\n",
        "\n",
        "$$ \\text{Cosine Similarity} = \\frac{\\text{TF-IDF vector 1} \\cdot \\text{TF-IDF vector 2}}{\\text{norm of TF-IDF vector 1} \\times \\text{norm of TF-IDF vector 2}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHwCgE4eDuee"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    # To do deal with the words that are not in the vocabulary you need to check if the word is in the vocabulary.\n",
        "    dot_product = sum([vec1[word] * vec2[word] for word in vec1 if word in vec2])\n",
        "\n",
        "    \"\"\"\n",
        "    you need to implement the computation of the magnitude of the vectors here.\n",
        "\n",
        "    vec1_norm =\n",
        "    vec2_norm =\n",
        "    \"\"\"\n",
        "\n",
        "    if not vec1_norm or not vec2_norm: # if one of the vectors is empty return 0\n",
        "        return 0\n",
        "\n",
        "    cosine_sim = dot_product / (vec1_norm * vec2_norm)\n",
        "\n",
        "    return cosine_sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPxowgOCDuee"
      },
      "source": [
        "## 3. TF-IDF를 이용한 문서 분류하기\n",
        "\n",
        "TF-IDF와 Cosine Similarity를 이용하여 classification task를 다음과 같이 진행할 수 있다.\n",
        "\n",
        "1. Training set의 각 category의 문서들에 대해, TF-IDF를 구해 평균을 계산한다.\n",
        "2. Inference시, test set의 instance에 대해, TF-IDF를 구한다.\n",
        "3. Training set의 각 category의 평균 TF-IDF와 test set의 instance의 TF-IDF를 이용하여, 코사인 유사도를 계산한다.\n",
        "4. 가장 높은 코사인 유사도를 가진 category로 분류한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldp0sN1MDuef"
      },
      "outputs": [],
      "source": [
        "def compute_class_avg_tfidf(data):\n",
        "    class_avg_tfidf = {}\n",
        "    class_counts = {}\n",
        "\n",
        "    \"\"\"\n",
        "    you need to implement the computation of the class average TF-IDF here.\n",
        "    for instance in data:\n",
        "        category =\n",
        "        tfidf =\n",
        "\n",
        "        if category not in class_avg_tfidf:\n",
        "            ~~\n",
        "        else :\n",
        "            ~~\n",
        "\n",
        "    for category in class_avg_tfidf:\n",
        "        for word in class_avg_tfidf[category]:\n",
        "            # normalize the class average TF-IDF\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return class_avg_tfidf\n",
        "\n",
        "class_avg_tfidf = compute_class_avg_tfidf(train_data)\n",
        "\n",
        "print(class_avg_tfidf['business']['music'])\n",
        "print(class_avg_tfidf['entertainment']['music'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WY2CwgRDuef"
      },
      "outputs": [],
      "source": [
        "def classify(instance, class_avg_tfidf):\n",
        "    instance_tfidf = instance['tfidf']\n",
        "    category_scores = {category: cosine_similarity(instance_tfidf, class_avg_tfidf[category]) for category in class_avg_tfidf}\n",
        "    return max(category_scores, key=category_scores.get)\n",
        "\n",
        "def evaluate(test_data, class_avg_tfidf):\n",
        "    correct = 0\n",
        "    for instance in test_data:\n",
        "        predicted = classify(instance, class_avg_tfidf)\n",
        "        if predicted == instance['category']:\n",
        "            correct += 1\n",
        "    return correct / float(len(test_data))\n",
        "\n",
        "print(evaluate(test_data, class_avg_tfidf))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cpwan",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}