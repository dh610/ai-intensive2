{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dh610/ai-intensive2/blob/main/lab3/lab3_Word%20Embedding_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/MyDrive/ai-intensive2\n",
        "!git pull\n",
        "%cd lab3"
      ],
      "metadata": {
        "id": "ByW_BcR8EsYI",
        "outputId": "fdb0ebf9-f3e4-4ea5-871f-aff512f1b10f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ai-intensive2\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 2.88 KiB | 1024 bytes/s, done.\n",
            "From https://github.com/dh610/ai-intensive2\n",
            "   7a560a6..94724d5  main       -> origin/main\n",
            "Updating 7a560a6..94724d5\n",
            "Fast-forward\n",
            " lab3/lab3_TF-IDF_assignment.ipynb | 918 \u001b[32m+++++++++++++++++++++++++++++++++\u001b[m\u001b[31m-------------------------\u001b[m\n",
            " 1 file changed, 531 insertions(+), 387 deletions(-)\n",
            "/content/drive/MyDrive/ai-intensive2/lab3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6HfJq3sEqa-"
      },
      "source": [
        "# Lab 3 : Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Mdp70JEqbC"
      },
      "source": [
        "@copyright:\n",
        "    (c) 2023. iKnow Lab. Ajou Univ., All rights reserved.\n",
        "\n",
        "M.S. Student: Wansik-Jo (jws5327@ajou.ac.kr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKMgP8cLEqbD"
      },
      "source": [
        "# For assignment\n",
        "\n",
        "- Python code의 주석 처리되어있는 부분을 구현하면 됩니다.\n",
        "- MD 형식의 Cell의 [BLANK] 부분을 채우면 됩니다.\n",
        "- MD 형식의 Cell의 [ANSWER] 부분 이후에 답을 작성하면 됩니다.\n",
        "- 조교에게 퀴즈의 답과 함께 코드 실행 결과를 보여준 뒤, BB에 제출 후 가시면 됩니다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMWkTo3pEqbH"
      },
      "source": [
        "\n",
        "## 목차\n",
        "\n",
        "1. Word Embeddings (Introduction)\n",
        "    - Word2vec\n",
        "    - Predict Capital of Country\n",
        "    - PCA\n",
        "2. CBOW vs Skip-gram\n",
        "    - CBOW\n",
        "    - Implementation from Scratch\n",
        "    - Skip-gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-i4MUt0EqbI"
      },
      "source": [
        "## 1. Word Embeddings (Introduction)\n",
        "\n",
        "### 1.1 google word2vec\n",
        "\n",
        "- word2vec은 2013년 구글 연구팀이 발표한 논문으로, 단어를 벡터로 표현하는 방법론이다.\n",
        "- word2vec은 CBOW와 Skip-gram 두 가지 방법론을 제시한다.\n",
        "\n",
        "- [BLANK]는 주변 단어들을 이용해 중심 단어를 예측하는 방법론이다.\n",
        "- [BLANK]은 중심 단어를 이용해 주변 단어들을 예측하는 방법론이다.\n",
        "\n",
        "해당 실습에서는, 학습된 word2vec 모델을 이용해 word vector를 얻어보고, 이를 이용해 단어 간 관계를 분석한다.\n",
        "\n",
        "<details>\n",
        "<summary>Reference</summary>\n",
        "\n",
        "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013.\n",
        "\n",
        "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n",
        "\n",
        "Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of NAACL HLT, 2013.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjxtXRzOEqbK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo1KH5wgEqbM"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('./data/capital_list.txt', delimiter=' ')\n",
        "data.columns = ['capital_s', 'country_s', 'capital_t', 'country_t']\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-ETiSYrEqbN"
      },
      "source": [
        "본 실습에서는, [google word2vec](https://code.google.com/archive/p/word2vec/) model을 이용한다.\n",
        "하지만, [The Original GoogleNews-vectors-negative300.bin.gz](https://code.google.com/archive/p/word2vec/).는 3.64GB로 용량이 매우 크기 때문에, 본 실습을 위하여 사용되는 몇 개의 단어에 대한 word vector만을 추출하여 사용한다.\n",
        "\n",
        "해당 추출 과정은 다음과 같은 과정으로 추출하여 제공하였다.\n",
        "\n",
        "<details>\n",
        "<summary>CODE 보기</summary>\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
        "\n",
        "f = open('capitals.txt', 'r').read()\n",
        "set_words = set(nltk.word_tokenize(f))\n",
        "assign_word = words = ['king', 'queen', 'oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n",
        "\n",
        "for w in assign_word:\n",
        "    set_words.add(w)\n",
        "\n",
        "def get_word_embeddings(embeddings):\n",
        "\n",
        "    word_embeddings = {}\n",
        "    for word in embeddings.vocab:\n",
        "        if word in set_words:\n",
        "            word_embeddings[word] = embeddings[word]\n",
        "    return word_embeddings\n",
        "\n",
        "pickle.dump( get_word_embeddings(embeddings), open( \"word_embeddings_subset.p\", \"wb\" ) )\n",
        "```\n",
        "</details>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRrP41WrEqbO"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "word_embeddings = pickle.load(open('./data/word_embeddings_subset.p', 'rb'))\n",
        "# print(word_embeddings.keys())\n",
        "# print(len(word_embeddings))\n",
        "# print(word_embeddings['country'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5bVgs31EqbP"
      },
      "source": [
        "## 1.2 Predict Capital of Country\n",
        "\n",
        "학습된 word2vec 모델을 이용해 단어 간 관계를 분석해보자.\n",
        "\n",
        "이를 위해, 강의노트에서 살펴본 예시에서와 같이 주어진 capital_list.txt file을 이용하여 단어간 관계를 계산하여 수도 이름을 예측하는 task를 해결하도록 한다.\n",
        "\n",
        "<br>\n",
        "\n",
        "먼저, 두 벡터간 유사도를 구할 수 있는 함수를 구현하고, Word2Vec 모델을 이용해 word embedding을 구하여 두 단어간 유사도를 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk66Ckt0EqbQ"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Implement cosine similarity between two vectors here.\n",
        "    You can just use the code from the previous lab.\n",
        "    \"\"\"\n",
        "\n",
        "    return cosine_similarity\n",
        "\n",
        "print(cosine_similarity(word_embeddings['Paris'], word_embeddings['France']))\n",
        "print(cosine_similarity(word_embeddings['China'], word_embeddings['France']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4-aIk54EqbR"
      },
      "source": [
        "다음으로, task를 해결하기 위해 구현해야하는 함수는 다음과 같다.\n",
        "\n",
        "- Function은 3가지의 input word(string)를 받는다.\n",
        "    * 첫번째 input word는 source country 이다.\n",
        "    * 두번째 input word는 source capital 이다. 즉, 첫번째 input word의 수도이다.\n",
        "    * 세번째 input word는 target country 이다. 즉, 알고자하는 수도의 국가이다.\n",
        "\n",
        "</br>\n",
        "\n",
        "- Function은 1가지의 output word(string)를 반환한다.\n",
        "    * output word는 target country의 수도이다.\n",
        "\n",
        "</br>\n",
        "\n",
        "- Function은 word embedding을 활용하기 위해 사진 학습된 word2vec 모델을 input으로 받는다.\n",
        "\n",
        "</br>\n",
        "\n",
        "- Function은 다음과 같은 방식으로 작동한다.\n",
        "    * source capital embedding에서 source country embedding을 뺀다.\n",
        "    * 위에서 구한 벡터에 target country embedding을 더한다.\n",
        "    * 위에서 구한 벡터와 가장 유사한 단어를 찾는다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcyDyoPrEqbS"
      },
      "outputs": [],
      "source": [
        "def predict_country(source_capital, source_country, target_country, word_embeddings):\n",
        "    \"\"\"\n",
        "    Implement the prediction function that described in the above cell.\n",
        "\n",
        "    source_capital =\n",
        "    source_country =\n",
        "    target_country =\n",
        "\n",
        "    predicted_capital_embedding =\n",
        "\n",
        "    max_similarity = -1\n",
        "    country = ''\n",
        "    for word, embedding in word_embeddings.items():\n",
        "        ~~\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return country\n",
        "\n",
        "print(predict_country('Paris', 'France', 'Italy', word_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJxwJjf8EqbT"
      },
      "outputs": [],
      "source": [
        "#Eval\n",
        "def compute_accuracy(word_embeddings, data):\n",
        "    num_correct = 0\n",
        "    for _, row in data.iterrows():\n",
        "        source_capital = row['capital_s']\n",
        "        source_country = row['country_s']\n",
        "        target_country = row['country_t']\n",
        "\n",
        "        predicted_capital = predict_country(source_capital, source_country, target_country, word_embeddings)\n",
        "\n",
        "        if predicted_capital == row['capital_t']:\n",
        "            num_correct += 1\n",
        "\n",
        "    accuracy = num_correct / len(data)\n",
        "    return accuracy\n",
        "\n",
        "print(compute_accuracy(word_embeddings, data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bqlf92ZEqbT"
      },
      "source": [
        "### 1.3 PCA\n",
        "\n",
        "Word2Vec을 통한 word간 embedding 수준에서의 distance를 plotting하여 살펴보자.\n",
        "\n",
        "현재 word2vec 모델은 [BLANK]차원의 vector를 이용하여 단어를 표현하고 있다. 따라서, 이를 인간이 이해할 수 있는 [BLANK]차원으로 축소하여 plotting을 진행한다.\n",
        "\n",
        "이를 위해, [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)를 이용하여 [BLANK]차원의 vector를 [BLANK]차원으로 축소한다.\n",
        "\n",
        "\n",
        "PCA를 계산하는 방법은, 다음과 같다.\n",
        "\n",
        "1. Data를 mean으로 normalize한다.\n",
        "2. Covariance matrix를 구한다.\n",
        "3. Covariance matrix의 eigenvalue와 eigenvector를 구한다.\n",
        "4. K개의 eigenvector를 선택한다.\n",
        "5. K개의 eigenvector를 이용해 K차원의 vector로 data를 projection한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTO6BykWEqbU"
      },
      "outputs": [],
      "source": [
        "def compute_PCA(X, n_components=2):\n",
        "    X_demeaned = X - np.mean(X, axis=0)\n",
        "    cov_matrix = np.cov(X_demeaned, rowvar=False)\n",
        "    eigen_values, eigen_vectors = np.linalg.eigh(cov_matrix)\n",
        "    idx_sorted = np.argsort(eigen_values)[::-1]\n",
        "    eigen_values_sorted = eigen_values[idx_sorted]\n",
        "    eigen_vectors_sorted = eigen_vectors[:, idx_sorted]\n",
        "    eigen_vectors_subset = eigen_vectors_sorted[:, 0:n_components]\n",
        "    X_reduced = np.dot(eigen_vectors_subset.transpose(), X_demeaned.transpose()).transpose()\n",
        "    return X_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT_OkyhTEqbV"
      },
      "outputs": [],
      "source": [
        "word_list = ['China', 'Italy', 'France', 'Berlin', 'Rome', 'Paris', 'Beijing', 'Germany', 'Japan', 'Tokyo', 'town', 'city', 'village']\n",
        "\n",
        "X = np.zeros((1, 300))\n",
        "for word in word_list:\n",
        "    X = np.row_stack((X, word_embeddings[word]))\n",
        "X = X[1:, :]\n",
        "\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t86H7q0EqbW"
      },
      "outputs": [],
      "source": [
        "X_reduced = compute_PCA(X, n_components=2)\n",
        "plt.scatter(X_reduced[:, 0], X_reduced[:, 1])\n",
        "for i, word in enumerate(word_list):\n",
        "    plt.annotate(word, xy=(X_reduced[i, 0], X_reduced[i, 1]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttv3kcNQEqbW"
      },
      "source": [
        "## 2. CBOW vs Skip-gram\n",
        "\n",
        "- [BLANK]: Predict context words (surrounding words) given target word\n",
        "- [BLANK]: Predict target word from bag of context words\n",
        "\n",
        "예를 들어, 다음과 같은 문장이 있다고 하자.\n",
        "\n",
        "$$ \\text{\"I am happy because I am learning\"} $$\n",
        "\n",
        "Context size가 2라고 하면\n",
        "\n",
        "[BLANK]의 경우, target word \"happy\"를 예측하기 위해 \"I\", \"am\", \"because\", \"I\"를 이용한다.\n",
        "\n",
        "반면, [BLANK]의 경우, target word \"happy\"를 통해 \"I\", \"am\", \"because\", \"I\"를 예측한다.\n",
        "\n",
        "***\n",
        "\n",
        "### 2.1 CBOW\n",
        "\n",
        "CBOW 학습 과정에 대해 자세히 알아보자.\n",
        "\n",
        "위 예시에서의 경우, CBOW는 다음과 같이 학습된다.\n",
        "\n",
        "$$ \\text{Context = [\"I\", \"am\", \"because\", \"I\"]} $$\n",
        "$$ \\text{Target = [\"happy\"]} $$\n",
        "\n",
        "$$ \\text{Input = [\"I\", \"am\", \"because\", \"I\"]} $$\n",
        "$$ \\text{Output = [\"happy\"]} $$\n",
        "\n",
        "이때, 모델 구조는 다음과 같다.\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='https://github.com/dh610/ai-intensive2/blob/main/lab3/data/word2.png?raw=1' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:600px;height:250px;\" />  </div>\n",
        "\n",
        "이때, $\\bar x$는 input context word들의 one-hot vector의 평균을 의미한다.\n",
        "\n",
        "$$ \\bar x = \\frac{1}{4} \\sum_{i=1}^{4} x_i $$\n",
        "\n",
        "최종적으로 정리하자면,\n",
        "\n",
        "$$ h = W_{1}X + b_1 $$\n",
        "$$ a = ReLU(h) $$\n",
        "$$ z = W_{2}a + b_2 $$\n",
        "$$ \\hat y = softmax(z) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MOjBXePEqbX"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De9NDVbuEqbY"
      },
      "outputs": [],
      "source": [
        "with open('./data/shakespeare.txt') as f:\n",
        "    corpus = f.read()\n",
        "\n",
        "\"\"\"\n",
        "In here, you should implement preprocessing step\n",
        "1. Replace all non-alphabetic characters with space (you can use regex or isalpha() function either)\n",
        "2. Replece all punctuations(, ! ? ; -) with period (you should use regex)\n",
        "3. Tokenize the corpus (you can use nltk.word_tokenize() function)\n",
        "\n",
        "like..\n",
        "\n",
        "corpus = re.sub(~~~~)\n",
        "\n",
        "This preprocessing step is very important to get good performance influencing on after all steps.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nofh8P9fEqbZ"
      },
      "outputs": [],
      "source": [
        "freq_dist = nltk.FreqDist(word for word in corpus) #Means frequency distribution\n",
        "print(len(freq_dist))\n",
        "print(freq_dist.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2UEQbAPEqba"
      },
      "outputs": [],
      "source": [
        "words = sorted(list(set(corpus)))\n",
        "idx = 0\n",
        "word2idx, idx2word = {}, {}\n",
        "for x in words:\n",
        "    word2idx[x] = idx\n",
        "    idx2word[idx] = x\n",
        "    idx += 1\n",
        "\n",
        "print(word2idx['apple'])\n",
        "print(idx2word[0])\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASO5YOWJEqba"
      },
      "source": [
        "### 2.2 Training model\n",
        "\n",
        "CBOW 모델을 학습시키기 위해, 다음과 같은 과정을 따라간다.\n",
        "\n",
        "- 1. Model Layer Initialization\n",
        "- 2. Forward Propagation\n",
        "- 3. Loss Function\n",
        "- 4. Backward Propagation\n",
        "- 5. Gradient Descent\n",
        "\n",
        "먼저, Model Layer Initialization을 진행한다.\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='https://github.com/dh610/ai-intensive2/blob/main/lab3/data/word2.png?raw=1' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:600px;height:250px;\" />  </div>\n",
        "\n",
        "- 첫번째 matrix $W_1$은 $N \\times V$의 크기를 가진다.\n",
        "    * $N$은 word vector의 차원이다.\n",
        "    * $V$는 vocabulary size이다.\n",
        "\n",
        "\n",
        "- 두번째 matrix $W_2$는 $V \\times N$의 크기를 가진다.\n",
        "    * $V$는 vocabulary size이다.\n",
        "    * $N$은 word vector의 차원이다.\n",
        "\n",
        "<br>\n",
        "\n",
        "- 첫번째 bias $b_1$은 [BLANK]의 크기를 가진다.\n",
        "    \n",
        "- 두번째 bias $b_2$는 [BLANK]의 크기를 가진다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDLjb7f4Eqba"
      },
      "outputs": [],
      "source": [
        "def init_model(N, V):\n",
        "    np.random.seed(42)\n",
        "\n",
        "    W1 = np.random.rand(N, V)\n",
        "    W2 = np.random.rand(V, N)\n",
        "\n",
        "    b1 = np.zeros((N, 1))\n",
        "    b2 = np.zeros((V, 1))\n",
        "\n",
        "    return W1, W2, b1, b2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvDVVZV3Eqbb"
      },
      "source": [
        "Softmax\n",
        "\n",
        "$$ \\text{softmax}(z_i) = \\frac{e^{z_i} }{\\sum_{i=0}^{V-1} e^{z_i} }  \\tag{5} $$\n",
        "\n",
        "- V는 vocabulary size이다.\n",
        "- $z_i$는 $i$번째 단어의 score이다.\n",
        "- i는 0부터 V-1까지의 단어를 의미한다. (idx가 0부터 시작)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMEmdhRjEqbb"
      },
      "outputs": [],
      "source": [
        "def softmax(z):\n",
        "    e_z = np.exp(z)\n",
        "    return e_z / np.sum(e_z, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq3v2rGEEqbc"
      },
      "source": [
        "### Forward\n",
        "\n",
        "Forward propagation은 다음과 같이 진행할 수 있다.\n",
        "\n",
        "$$ h = W_{1}X + b_1 $$\n",
        "$$ a = ReLU(h) $$\n",
        "$$ z = W_{2}a + b_2 $$\n",
        "\n",
        "이때, ReLU function은 다음과 같이 정의된다.\n",
        "\n",
        "$$ ReLU(x) = \\max(0,x) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaS3xYeuEqbd"
      },
      "outputs": [],
      "source": [
        "def ReLU(x):\n",
        "    \"\"\"\n",
        "    Implement own ReLU function here\n",
        "    You can use np library\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76M7tOWMEqbd"
      },
      "outputs": [],
      "source": [
        "def forward(x, W1, W2, b1, b2):\n",
        "    \"\"\"\n",
        "    You should implement forward function that described in the above cell.\n",
        "    h =\n",
        "    a =\n",
        "    z =\n",
        "    \"\"\"\n",
        "\n",
        "    return z, a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRvmQvqdEqbe"
      },
      "source": [
        "### Loss\n",
        "\n",
        "Cost function은 다음과 같이 cross entropy loss function을 이용한다.\n",
        "\n",
        "$$ cross\\_entropy(y, \\hat y) = - \\sum_{i=0}^{V-1} [ y_i \\log(\\hat y_i) + (1-y_i) \\log(1-\\hat y_i) ]$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlBHQCqWEqbe"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(y, y_hat, batch_size):\n",
        "    \"\"\"\n",
        "    You should implement cross entropy loss function that described in the above cell. (just one line)\n",
        "    log_probs =\n",
        "\n",
        "    I provide you summation part here, with the shape of batch size\n",
        "    cost = -1/batch_size * np.sum(log_probs)\n",
        "    cost = np.squeeze(cost) #squeeze is used for removing single-dimensional entries from the shape of an array.\n",
        "    \"\"\"\n",
        "\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-PmL4ZkEqbf"
      },
      "source": [
        "### Backpropagation\n",
        "\n",
        "지금까지 CBOW model이 어떻게 동작하는지 학습했다.\n",
        "모델을 구조를 Initalize하고, Forward 함수를 정의하였으며, Cost functinon 또한 정의하였다.\n",
        "\n",
        "이제, Gradient를 계산하여 Back-propagation을 진행하자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k8hMk_JEqbf"
      },
      "outputs": [],
      "source": [
        "def back_propagation(x, y_hat, y, h, W1, W2, b1, b2, batch_size):\n",
        "    l1 = np.dot(W2.T, (y_hat - y)) #l1 means layer1\n",
        "    l1 = ReLU(l1)\n",
        "\n",
        "    grad_W1 = (1/batch_size) * np.dot(l1, x.T)\n",
        "    grad_W2 = (1/batch_size) * np.dot(y_hat - y, h.T)\n",
        "\n",
        "    grad_b1 = np.sum((1/batch_size) * np.dot(l1, x.T), axis=1, keepdims=True)\n",
        "    grad_b2 = np.sum((1/batch_size) * np.dot(y_hat-y, h.T), axis=1, keepdims=True)\n",
        "\n",
        "    return grad_W1, grad_W2, grad_b1, grad_b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02kyoP8sEqbg"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "!NOTE\n",
        "This cell provide you just useful functions for training to deal with batch.\n",
        "You don't need to implement anything in here.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def pack_idx_with_frequency(context_words, word2idx):\n",
        "    freq_dict = defaultdict(int)\n",
        "    for word in context_words:\n",
        "        freq_dict[word] += 1\n",
        "    idxs = get_idx(context_words, word2idx)\n",
        "    packed = []\n",
        "    for i in range(len(idxs)):\n",
        "        idx = idxs[i]\n",
        "        freq = freq_dict[context_words[i]]\n",
        "        packed.append((idx, freq))\n",
        "    return packed\n",
        "\n",
        "def get_idx(words, word2idx):\n",
        "    idx = []\n",
        "    for word in words:\n",
        "        idx = idx + [word2idx[word]]\n",
        "    return idx\n",
        "\n",
        "def get_vectors(data, word2idx, V, C):\n",
        "    i = C\n",
        "    while True:\n",
        "        y = np.zeros(V)\n",
        "        x = np.zeros(V)\n",
        "        center_word = data[i]\n",
        "        y[word2idx[center_word]] = 1\n",
        "        context_words = data[(i - C):i] + data[(i+1):(i+C+1)]\n",
        "        num_ctx_words = len(context_words)\n",
        "        for idx, freq in pack_idx_with_frequency(context_words, word2idx):\n",
        "            x[idx] = freq/num_ctx_words\n",
        "        yield x, y\n",
        "        i += 1\n",
        "        if i >= len(data):\n",
        "            print('i is being set to 0')\n",
        "            i = 0\n",
        "\n",
        "def get_batches(data, word2idx, V, C, batch_size):\n",
        "    batch_x = []\n",
        "    batch_y = []\n",
        "    for x, y in get_vectors(data, word2idx, V, C):\n",
        "        while len(batch_x) < batch_size:\n",
        "            batch_x.append(x)\n",
        "            batch_y.append(y)\n",
        "        else:\n",
        "            yield np.array(batch_x).T, np.array(batch_y).T\n",
        "\n",
        "def get_dict(data):\n",
        "    words = sorted(list(set(data)))\n",
        "    n = len(words)\n",
        "    idx = 0\n",
        "\n",
        "    word2idx = {}\n",
        "    idx2word = {}\n",
        "    for k in words:\n",
        "        word2idx[k] = idx\n",
        "        idx2word[idx] = k\n",
        "        idx += 1\n",
        "    return word2idx, idx2word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdy63RuOEqbh"
      },
      "source": [
        "### Gradient Descent\n",
        "\n",
        "Backpropagation을 통해 gradient를 계산할 수 있게 되었다.\n",
        "\n",
        "이제, gradient descent를 통해 parameter를 update할 수 있도록 한다.\n",
        "\n",
        "그리고 해당 과정을 통해 모델의 학습을 진행하고, loss를 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLU7GGDDEqbh"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(data, word2idx, V, N, num_iters, batch_size, window_size):\n",
        "    iters = 0\n",
        "    \"\"\"\n",
        "    You should implement gradient descent algorithm here.\n",
        "    You use all of the functions that you have implemented above.\n",
        "\n",
        "    init model\n",
        "\n",
        "    for x, y in get_batches(data, word2idx, V, window_size, batch_size):\n",
        "        z, h =\n",
        "        y_hat =\n",
        "\n",
        "        cost =\n",
        "        if ((iters+1) % 10 == 0): #This line is for printing cost every 10 iterations < DO NOT CHANGE THIS LINE >\n",
        "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
        "\n",
        "        do back-propagation\n",
        "\n",
        "        iters += 1\n",
        "        if iters == num_iters:\n",
        "            break\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return W1, W2, b1, b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hVWsHSEEqbi"
      },
      "outputs": [],
      "source": [
        "word2idx, idx2word = get_dict(corpus)\n",
        "\n",
        "W1, W2, b1, b2 = gradient_descent(corpus, word2idx, len(word2idx), N=50, num_iters=300,  batch_size=128, window_size=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfEihEEnEqbi"
      },
      "source": [
        "## 3.과제\n",
        "1) 구현한 CBOW Word2vVec Model의 Inference Setting, Hyperparameter를 수정하여, loss값을 조정하고, 해당 현상을 설명하세요.\n",
        "2) 구현한 CBOW Word2Vec Model을 Skip-gram Model로 수정하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2PyLWthEqbl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cpwan",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}