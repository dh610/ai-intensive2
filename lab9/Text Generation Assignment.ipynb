{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dh610/ai-intensive2/blob/main/lab9/Text%20Generation%20Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/ai-intensive2\n",
        "!git pull\n",
        "%cd lab9"
      ],
      "metadata": {
        "id": "ulLhIMDQ5az7",
        "outputId": "15c0b432-d071-4a32-e0ef-81c06cb3a447",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ai-intensive2\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 14.61 KiB | 26.00 KiB/s, done.\n",
            "From https://github.com/dh610/ai-intensive2\n",
            "   f54b27c..d373c74  main       -> origin/main\n",
            "Updating f54b27c..d373c74\n",
            "Fast-forward\n",
            " lab9/In-Context-Learning.ipynb | 3983 \u001b[32m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 3982 insertions(+), 1 deletion(-)\n",
            "/content/drive/MyDrive/ai-intensive2/lab9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWrW1CGB2pln"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 생성"
      ],
      "metadata": {
        "id": "wZn88UY422wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "id": "13AISBygkEb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05436d46-7909-4a0b-ca21-71223e848871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7c91111e8490>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_id = \"skt/kogpt2-base-v2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id).to(device).eval()"
      ],
      "metadata": {
        "id": "87obgehXlrTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Greedy generation\n",
        "\n",
        "가장 높은 확률의 토큰을 선택한다"
      ],
      "metadata": {
        "id": "PL2KwbJA3DjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "input_text = \"안녕하세요. \"\n",
        "\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
        "\n",
        "pprint(input_ids[\"input_ids\"])\n",
        "pprint(tokenizer.batch_decode(input_ids[\"input_ids\"], skip_special_tokens=False))\n",
        "\n",
        "logits = model(input_ids[\"input_ids\"]).logits\n",
        "\n",
        "pprint(logits.shape)\n",
        "\n",
        "next_tokens = logits[:, -1].argmax(-1)\n",
        "\n",
        "pprint(next_tokens)\n",
        "pprint(tokenizer.batch_decode(next_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vwCnkPz3BuY",
        "outputId": "fc3fd04d-14dd-44e4-e41e-eb1c04d1b9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[25906,  8702,  7801, 25856,   739]])\n",
            "['안녕하세요. ']\n",
            "torch.Size([1, 5, 51200])\n",
            "tensor([214])\n",
            "['^^']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multinomial Sampling\n",
        "각 확률을 가중치로 사용해서 확률적으로 추출한다"
      ],
      "metadata": {
        "id": "TCUbkRcW4R74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(torch.multinomial(torch.tensor([0.1, 0.5, 0.4]), 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fax4vLBk4nIZ",
        "outputId": "3bc2a776-e753-49cb-92d3-769b1f4457a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n",
            "tensor([1])\n",
            "tensor([2])\n",
            "tensor([1])\n",
            "tensor([2])\n",
            "tensor([2])\n",
            "tensor([0])\n",
            "tensor([1])\n",
            "tensor([1])\n",
            "tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  next_tokens = torch.multinomial(logits[:, -1].softmax(-1), 1)\n",
        "  pprint(next_tokens)\n",
        "  pprint(tokenizer.batch_decode(next_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmtpSrko4ZQq",
        "outputId": "d7ab3269-a80f-4da0-b3be-9dcfdb981955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[214]])\n",
            "['^^']\n",
            "tensor([[7735]])\n",
            "['뽀']\n",
            "tensor([[216]])\n",
            "['ㅠㅠ']\n",
            "tensor([[7522]])\n",
            "['멕']\n",
            "tensor([[7570]])\n",
            "['뭘']\n",
            "tensor([[214]])\n",
            "['^^']\n",
            "tensor([[9705]])\n",
            "['..']\n",
            "tensor([[605]])\n",
            "['ᄏ']\n",
            "tensor([[6910]])\n",
            "['괜']\n",
            "tensor([[214]])\n",
            "['^^']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top-K sampling\n",
        "\n",
        "확률이 가장 높은 K개를 골라서 그 중에서 샘플링한다\n",
        "\n",
        "1. top-k token을 얻어온다\n",
        "2. top-k token을 제외한 나머지 값들의 확률을 모두 -inf으로 만든다(극단적인 값으로 낮춤)\n",
        "3. 이후 샘플링하면 끝\n",
        "\n",
        "2번을 하는 방법\n",
        "k번째 확률값을 가져온다\n"
      ],
      "metadata": {
        "id": "G-jA9Zvz48GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.tensor([\n",
        "          [0.5, 0.3, 0.4, 0.1],\n",
        "          [0.9, 0.1, 0.01, 0.5]\n",
        "          ])\n",
        "k = 2\n",
        "values, indices = A.topk(k)\n",
        "k_value = values[:, -1].unsqueeze(-1)\n",
        "print(k_value)\n",
        "print(A < k_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V5RZv9hFHIl",
        "outputId": "d410a553-80d4-4716-c212-c6f5ed60e971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4000],\n",
            "        [0.5000]])\n",
            "tensor([[False,  True, False,  True],\n",
            "        [False,  True,  True, False]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_topk(logits, k=50):\n",
        "  logits = logits.clone()\n",
        "  lowest_value = - float('Inf')\n",
        "\n",
        "  values, indices = logits.topk(k)\n",
        "\n",
        "  # k 번째로 높은 값의 value를 가져온다\n",
        "  k_value = values[:, -1].unsqueeze(-1) # [batch, 1]\n",
        "\n",
        "  # k_value 보다 낮은 확률값은 전부 lowest_value 로 만들어버리면 끝!\n",
        "  logits = logits.masked_fill(logits < k_value, lowest_value)\n",
        "  return logits\n",
        "\n",
        "\n",
        "for k in [1, 2, 3]:\n",
        "  print(f\"k = {k}\", filter_topk (\n",
        "      torch.tensor([\n",
        "          [0.5, 0.3, 0.4, 0.1],\n",
        "          [0.9, 0.1, 0.01, 0.5]\n",
        "          ]),\n",
        "      k = k\n",
        "  ), sep=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4oaHEut4ewK",
        "outputId": "ef981f4c-45b1-45bd-b823-efa83067fcef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k = 1\n",
            "tensor([[0.5000,   -inf,   -inf,   -inf],\n",
            "        [0.9000,   -inf,   -inf,   -inf]])\n",
            "k = 2\n",
            "tensor([[0.5000,   -inf, 0.4000,   -inf],\n",
            "        [0.9000,   -inf,   -inf, 0.5000]])\n",
            "k = 3\n",
            "tensor([[0.5000, 0.3000, 0.4000,   -inf],\n",
            "        [0.9000, 0.1000,   -inf, 0.5000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ㄴ 위 함수 정답 결과는 아래와 같아야 합니다\n",
        "```\n",
        "k = 1\n",
        "tensor([[0.5000,   -inf,   -inf,   -inf],\n",
        "        [0.9000,   -inf,   -inf,   -inf]])\n",
        "k = 2\n",
        "tensor([[0.5000,   -inf, 0.4000,   -inf],\n",
        "        [0.9000,   -inf,   -inf, 0.5000]])\n",
        "k = 3\n",
        "tensor([[0.5000, 0.3000, 0.4000,   -inf],\n",
        "        [0.9000, 0.1000,   -inf, 0.5000]])\n",
        "```"
      ],
      "metadata": {
        "id": "PbTB2Xpt6rdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  next_logits = filter_topk(logits[:, -1], 50)\n",
        "\n",
        "  next_tokens = torch.multinomial(next_logits.softmax(-1), 1)\n",
        "  pprint(next_tokens)\n",
        "  pprint(tokenizer.batch_decode(next_tokens))"
      ],
      "metadata": {
        "id": "J9bXesrq6UDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nucleus Sampling (Top-p sampling)\n",
        "Top-K 샘플링의 단점은, 확률분포에 따라 너무 확률이 작은 토큰도 top-k에 포함될 위험도 있다는 점입니다. 이를 보안하기 위해 top-p sampling이 제안됬습니다. 이 방법은 토큰을 확률 순으로 정렬한 다음, 확률들을 큰 값부터 더해가다가 확률의 합이 threshold를 넘을 경우 중단하고 그 전의 높은 확률의 토큰들만을 샘플링에 사용합니다.\n",
        "\n",
        "\n",
        "```\n",
        "예를 들어 아래와 같은 확률이 있을 때\n",
        "[0.05, 0.5, 0.3, 0.15]\n",
        "\n",
        "정렬하면\n",
        "[0.5, 0.3, 0.15, 0.05]\n",
        "\n",
        "누적합\n",
        "[0.5, 0.8, 0.95, 1.0]\n",
        "\n",
        "threshold = 0.6일 경우 (0.6을 초과하는 토큰까지 포함)\n",
        "[0.5, 0.3]\n",
        "\n",
        "threshold = 0.8일 경우 (0.8과 정확히 일치해도 그 이후 토큰까지 포함함)\n",
        "[0.5, 0.3, 0.15]\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "SVQ5uT-N8qfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint 1. torch.sort & torch.cumsum\n",
        "A = torch.tensor([\n",
        "    [0.05, 0.5, 0.3, 0.15],\n",
        "    [0.7, 0.09, 0.2, 0.01]\n",
        "    ])\n",
        "\n",
        "sorted_logits, sorted_indices = torch.sort(A, descending=True) # 내림차순 정렬\n",
        "print(\"sorted_logits:\", sorted_logits)\n",
        "print(\"sorted_indices:\", sorted_indices)\n",
        "\n",
        "# dim=1 으로 누적\n",
        "cum_logits = torch.cumsum(sorted_logits, dim=-1)\n",
        "print(\"cum_logits:\", cum_logits)\n",
        "\n",
        "target = cum_logits >= 0.6\n",
        "print(target)\n",
        "#shift\n",
        "target[:, 1:] = target[:, :-1].clone()\n",
        "target[:, 0] = False\n",
        "print(target)\n",
        "\n",
        "# dim=0 으로 누적\n",
        "cum_logits = torch.cumsum(sorted_logits, dim=0)\n",
        "print(\"cum_logits:\", cum_logits)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxZmaUN6rK6X",
        "outputId": "bce4d3e8-3f21-48b5-8ffd-2bf5dd201b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sorted_logits: tensor([[0.5000, 0.3000, 0.1500, 0.0500],\n",
            "        [0.7000, 0.2000, 0.0900, 0.0100]])\n",
            "sorted_indices: tensor([[1, 2, 3, 0],\n",
            "        [0, 2, 1, 3]])\n",
            "cum_logits: tensor([[0.5000, 0.8000, 0.9500, 1.0000],\n",
            "        [0.7000, 0.9000, 0.9900, 1.0000]])\n",
            "tensor([[False,  True,  True,  True],\n",
            "        [ True,  True,  True,  True]])\n",
            "tensor([[False, False,  True,  True],\n",
            "        [False,  True,  True,  True]])\n",
            "cum_logits: tensor([[0.5000, 0.3000, 0.1500, 0.0500],\n",
            "        [1.2000, 0.5000, 0.2400, 0.0600]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint 2. torch.scatter (src의 값을 index를 바꾸어서 저장한다)\n",
        "\n",
        "target = torch.zeros((1, 4), dtype=torch.int32) # [[0, 0, 0, 0]]\n",
        "print(target)\n",
        "\n",
        "target.scatter_(\n",
        "    dim=-1,\n",
        "    index=torch.tensor([\n",
        "        [3,1,2,0], # 새로 저장할 index\n",
        "        ]),\n",
        "    src=torch.tensor([[11,22,33,44]],dtype=torch.int32),\n",
        "    )\n",
        "\n",
        "print(target)"
      ],
      "metadata": {
        "id": "ewzXkdFStbfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def filter_topp(logits, threshold, verbose=False, return_softmax=True):\n",
        "  logits = logits.clone()\n",
        "  lowest_value = - float('Inf')\n",
        "  batch_size = logits.shape[0]\n",
        "\n",
        "  # 1. 정렬 후 cumsum 을 이용해서 누적합 계산\n",
        "  sorted_logits, sorted_indices = ??\n",
        "  cumulative_probs = ??\n",
        "\n",
        "  # 2. threshold 보다 누적합이 큰 구간을 찾는다.\n",
        "  # ex) threshold = 0.5\n",
        "  # before: [0.5000, 0.7500, 0.9000, 1.0000]\n",
        "  # after: [ False True True True]\n",
        "  if verbose:\n",
        "    print(\"cumulative_probs\", cumulative_probs)\n",
        "\n",
        "  sorted_indices_to_remove = ??\n",
        "\n",
        "  if verbose:\n",
        "    print(\"sorted_indices\", sorted_indices)\n",
        "    print(\"sorted_indices_to_remove\", sorted_indices_to_remove)\n",
        "\n",
        "  # 3. threshold를 초과한 토큰까지는 포함해야 하므로 sorted_indices_to_remove를 한 칸 옆으로 shift 한다\n",
        "  # before: [ False True True True]\n",
        "  # after: [ False False True True]\n",
        "  sorted_indices_to_remove ??\n",
        "\n",
        "  if verbose:\n",
        "    print(\"sorted_indices_to_remove (shifted)\", sorted_indices_to_remove)\n",
        "\n",
        "  #\n",
        "  # indices_to_remove 에는 cumsum이 threshold 를 넘는 토큰은 True 아닌 토큰은 False가 저장되어야 합니다.\n",
        "  # 예시) 처음 확률값: [0.5, 0.15, 0.25, 0.1]\n",
        "  # sorted_indices: [0, 2, 1, 3]\n",
        "  # cumulative_probs: [0.5000, 0.7500, 0.9000, 1.0000]\n",
        "  # sorted_indices_to_remove: [False, False,  True,  True]\n",
        "  #\n",
        "  # 앞에 두 값은 False고, 뒤의 두 갑은 True입니다.\n",
        "  # 즉 sorted_index에서 앞의 0,2 는 top_p에 포함되고, 뒤의 1, 3은 top_p에 포함되지 않는다는 의미입니다.\n",
        "  # 이제 scatter_를 이용해서 sorted_indices_to_remove 를 sorted_indices 맞춰 아래처럼 위치를 수정하면 됩니다.\n",
        "  # indices_to_remove: [False, True, False, True]\n",
        "  indices_to_remove = torch.zeros_like(logits, dtype=sorted_indices_to_remove.dtype)\n",
        "  indices_to_remove.scatter_( ??? )\n",
        "\n",
        "  if verbose:\n",
        "    print(\"indices_to_remove (scattered)\", indices_to_remove)\n",
        "\n",
        "  if return_softmax:\n",
        "    logits = logits.log()\n",
        "  # True 인 index에만 lowest_value를 적용하고, 다시 총합을 1로 만듭니다.\n",
        "  logits[indices_to_remove] = lowest_value\n",
        "\n",
        "  return logits.softmax(-1) if return_softmax else logits\n",
        "\n",
        "\n",
        "p = 0.70\n",
        "print(f\"p = {p}\", filter_topp (\n",
        "    torch.tensor([\n",
        "        [0.5, 0.15, 0.25, 0.1],\n",
        "        ]),\n",
        "    p,\n",
        "    True,\n",
        "    False\n",
        "), sep=\"\\n\")"
      ],
      "metadata": {
        "id": "GTvJXJDG9lWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음과 같은 결과가 나와야 합니다\n",
        "```\n",
        "cumulative_probs tensor([[0.5000, 0.7500, 0.9000, 1.0000]])\n",
        "sorted_indices tensor([[0, 2, 1, 3]])\n",
        "sorted_indices_to_remove tensor([[False,  True,  True,  True]])\n",
        "sorted_indices_to_remove (shifted) tensor([[False, False,  True,  True]])\n",
        "indices_to_remove (scattered) tensor([[False,  True, False,  True]])\n",
        "p = 0.7\n",
        "tensor([[0.5000,   -inf, 0.2500,   -inf]])\n",
        "```"
      ],
      "metadata": {
        "id": "NSPxxCNGxrDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test code!\n",
        "for p in [0.5, 0.75, 0.9, 0.99]:\n",
        "  print(f\"p = {p}\", filter_topp (\n",
        "      torch.tensor([\n",
        "          [0.5, 0.25, 0.15, 0.1],\n",
        "          [0.7, 0.01, 0.01, 0.28]\n",
        "          ]),\n",
        "      p,\n",
        "      False,\n",
        "      False\n",
        "  ), sep=\"\\n\")"
      ],
      "metadata": {
        "id": "TesiBg2RuiTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "실행결과\n",
        "\n",
        "```\n",
        "p = 0.5\n",
        "tensor([[0.5000, 0.2500,   -inf,   -inf],\n",
        "        [0.7000,   -inf,   -inf,   -inf]])\n",
        "p = 0.75\n",
        "tensor([[0.5000, 0.2500, 0.1500,   -inf],\n",
        "        [0.7000,   -inf,   -inf, 0.2800]])\n",
        "p = 0.9\n",
        "tensor([[0.5000, 0.2500, 0.1500, 0.1000],\n",
        "        [0.7000,   -inf,   -inf, 0.2800]])\n",
        "p = 0.99\n",
        "tensor([[0.5000, 0.2500, 0.1500, 0.1000],\n",
        "        [0.7000, 0.0100, 0.0100, 0.2800]])\n",
        "```"
      ],
      "metadata": {
        "id": "8nMwuZJNA7ng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 자기회귀 언어모델"
      ],
      "metadata": {
        "id": "QfrigMtSBHDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(prefix,\n",
        "             max_new_tokens,\n",
        "             temperature=1,\n",
        "             top_k=1,\n",
        "             top_p=0,\n",
        "             device='cpu'\n",
        "             ):\n",
        "    prefix_ids = tokenizer.encode(prefix, return_tensors=\"pt\").to(device)\n",
        "    past_key_values = None\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        outputs = model(\n",
        "            prefix_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            )\n",
        "        logits = outputs.logits\n",
        "        past_key_values = outputs.past_key_values\n",
        "\n",
        "        logits = logits[:, -1] / temperature\n",
        "        logits = filter_topk(logits, top_k)\n",
        "        logits = filter_topp(logits.softmax(-1), top_p)\n",
        "\n",
        "        next_tokens = torch.multinomial(logits, num_samples=1)\n",
        "\n",
        "        prefix_ids = torch.cat((prefix_ids, next_tokens), dim=1)\n",
        "\n",
        "    print(tokenizer.batch_decode(prefix_ids)[0])\n",
        "\n",
        "\n",
        "generate('안녕하세요~', 16, top_p=0.95, top_k=50, temperature=1.5)"
      ],
      "metadata": {
        "id": "FonZNBz6A_wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def model_generate(prefix,\n",
        "             max_new_tokens,\n",
        "             temperature=1,\n",
        "             top_k=1,\n",
        "             top_p=0,\n",
        "             device='cpu'\n",
        "             ):\n",
        "    prefix_ids = tokenizer.encode(prefix, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(prefix_ids, do_sample=True)\n",
        "    print(tokenizer.batch_decode(outputs)[0])\n",
        "\n",
        "model_generate('안녕하세요~', 16)"
      ],
      "metadata": {
        "id": "43TJBSunJnyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K/V Caching\n",
        "디코딩 과정에서, transformer block의 이전 step Key, Value는 다시계산할 필요가 없습니다. 따라서 이 값들은 캐싱해둔 뒤 계산에서 제외합니다. 이 방법을 KV caching이라고 부릅니다.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:720/1*8xqD4AYTwn6mQXNw0uhDCg.gif)\n",
        "before K/V caching\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:720/1*uyuyOW1VBqmF5Gtv225XHQ.gif)\n",
        "\n",
        "Reference: https://medium.com/@joaolages/kv-caching-explained-276520203249\n"
      ],
      "metadata": {
        "id": "okGXy9DlDuBD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FqIHPe33EJSy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}