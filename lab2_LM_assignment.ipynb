{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dh610/ai-intensive2/blob/main/lab2_LM_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/MyDrive/ai-intensive2"
      ],
      "metadata": {
        "id": "KXTiYOjqCCZm",
        "outputId": "d2ec6155-3816-4031-ab92-0da0f43c2389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'drive/MyDrive/ai-intensive2'\n",
            "/content/drive/MyDrive/ai-intensive2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WaFlV3sCBNw"
      },
      "source": [
        "# Lab 2 : Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tP4BdTTCBNz"
      },
      "source": [
        "@copyright:\n",
        "    (c) 2023. iKnow Lab. Ajou Univ., All rights reserved.\n",
        "\n",
        "M.S. Student: Wansik-Jo (jws5327@ajou.ac.kr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr8OBMV6CBN0"
      },
      "source": [
        "# For assignment\n",
        "\n",
        "- Python code의 주석 처리되어있는 부분을 구현하면 됩니다.\n",
        "- MD 형식의 Cell의 [BLANK] 부분을 채우면 됩니다.\n",
        "- MD 형식의 Cell의 [ANSWER] 부분 이후에 답을 작성하면 됩니다.\n",
        "- 조교에게 퀴즈의 답과 함께 코드 실행 결과를 보여준 뒤, BB에 제출 후 가시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEmAqwQuCBN0"
      },
      "source": [
        "\n",
        "# 목차\n",
        "1. [Language Modeling](##1.-Language-Modeling)\n",
        "2. [N-gram Language Model](##2.-N-gram-Language-Model)\n",
        "3. [Perplexity](##3.-Perplexity)\n",
        "4. [Generalization](##4.-Generalization)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnRtJpiSCBN1"
      },
      "source": [
        "## 1. Language Modeling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3btA67OCBN1"
      },
      "source": [
        "### 1.1. Language Modeling\n",
        "\n",
        "- 언어 모델링(Language Modeling)은 주어진 단어들의 나열에 대해 그 확률을 예측하는 작업이다.\n",
        "\n",
        "- 언어 모델은 다음과 같은 다양한 분야에서 활용된다.\n",
        "\n",
        "  - 기계 번역(Machine Translation)\n",
        "  - 오타 교정(Spell Correction)\n",
        "  - 음성 인식(Speech Recognition)\n",
        "  - 문장 자동 완성(Sentence Completion)\n",
        "  - 문장 유사도(Sentence Similarity)\n",
        "  - 감성 분석(Sentiment Analysis)\n",
        "  - 질의 응답(Question Answering)\n",
        "  - 챗봇(Chatbot)\n",
        "\n",
        "- 언어 모델링은 다음과 같은 다양한 방법으로 수행된다.\n",
        "\n",
        "    - N-gram Language Model\n",
        "    - Neural Network Language Model\n",
        "    - Transformer Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4vKBcDlCBN1"
      },
      "source": [
        "### 1.2 Markov Assumption\n",
        "\n",
        "- 언어 모델링은 다음과 같은 Markov Assumption을 기반으로 한다.\n",
        "\n",
        "    - 어떤 단어의 확률은 그 이전 단어들에만 의존한다.\n",
        "\n",
        "\n",
        "- 예를 들어, 다음 확률을 계산하고자 한다고 하자.\n",
        "\n",
        "$$ P(w_1, w_2, \\cdots, w_n) $$\n",
        "\n",
        "- 조건부 확률에 의해,\n",
        "\n",
        "$$ \\Pi_{i=1}^{n} P(w_i | w_1, w_2, \\cdots, w_{i-1}) $$\n",
        "\n",
        "- Chain Rule에 의해,\n",
        "\n",
        "$$ P(w_1, w_2, \\cdots, w_n) = P(w_1) P(w_2 | w_1) P(w_3 | w_1, w_2) \\cdots P(w_n | w_1, w_2, \\cdots, w_{n-1}) $$\n",
        "\n",
        "! 이는 모든 단어의 확률을 계산하기 위해 모든 이전 단어들을 고려해야 한다는 것을 의미한다. -> 비현실적\n",
        "\n",
        "- 따라서, Markov Assumption을 적용하면,\n",
        "\n",
        "$$ P(w_1, w_2, \\cdots, w_n) = \\Pi_{i=1}^{n} P(w_i | w_1, w_2, \\cdots, w_{i-1}) \\approx \\Pi_{i=1}^{n} P(w_i | w_{i-1}) $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gtPTu6OCBN2"
      },
      "source": [
        "## 2. N-gram Language Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-rolk1GCBN2",
        "outputId": "2cf5189d-9454-4a08-a26e-603a59c632d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['<s>', 'i'],\n",
              " ['i', 'do'],\n",
              " ['do', 'not'],\n",
              " ['not', 'like'],\n",
              " ['like', 'green'],\n",
              " ['green', 'eggs'],\n",
              " ['eggs', 'and'],\n",
              " ['and', 'ham'],\n",
              " ['ham', '</s>']]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def sentence_to_ngrams(sentence, n):\n",
        "    \"\"\"\n",
        "    Converts a sentence into a list of n-grams.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sentence : str\n",
        "        The sentence to convert.\n",
        "    n : int\n",
        "        The length of the n-grams.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        The list of n-grams.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    sentence = \"<s> \" + sentence + \" </s>\"\n",
        "    sentence = sentence.lower()\n",
        "    sentence = sentence.split()\n",
        "\n",
        "    ngrams = []\n",
        "    for i in range(len(sentence)-n+1):\n",
        "        ngrams.append(sentence[i:i+n])\n",
        "    return ngrams\n",
        "\n",
        "sentence_to_ngrams(\"I do not like green eggs and ham\", 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qOvzr21CBN4",
        "outputId": "6daa0d08-bbf5-4aa4-804f-dbef3cc6a921"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{('<s>', 'i', 'do'): 2,\n",
              " ('i', 'do', 'not'): 2,\n",
              " ('do', 'not', 'like'): 2,\n",
              " ('not', 'like', 'green'): 1,\n",
              " ('like', 'green', 'eggs'): 1,\n",
              " ('green', 'eggs', 'and'): 1,\n",
              " ('eggs', 'and', 'ham'): 1,\n",
              " ('and', 'ham', '</s>'): 1,\n",
              " ('not', 'like', 'them'): 1,\n",
              " ('like', 'them', 'sam'): 1,\n",
              " ('them', 'sam', 'i'): 1,\n",
              " ('sam', 'i', 'am'): 1,\n",
              " ('i', 'am', '</s>'): 1}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# n-gram LM\n",
        "\n",
        "def ngram_LM(n, train_data):\n",
        "    \"\"\"\n",
        "    Builds an n-gram language model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n : int\n",
        "        The length of the n-grams.\n",
        "    train_data : list\n",
        "        The training data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        The n-gram language model.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    ngram_lm = {}\n",
        "    for sentence in train_data:\n",
        "        ngrams = sentence_to_ngrams(sentence, n)\n",
        "        for ngram in ngrams:\n",
        "            ngram = tuple(ngram)\n",
        "            if ngram not in ngram_lm:\n",
        "                ngram_lm[ngram] = 1\n",
        "            else:\n",
        "                ngram_lm[ngram] += 1\n",
        "    return ngram_lm\n",
        "\n",
        "ngram_LM(3, [\"I do not like green eggs and ham\", \"I do not like them Sam I am\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5M5RZ90CBN4",
        "outputId": "3fdab287-4220-4b4b-f2bf-150aad78b654"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.125"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def ngram_prob(ngram, ngram_lm):\n",
        "    \"\"\"\n",
        "    Calculates the probability of an n-gram.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ngram : tuple\n",
        "        The n-gram.\n",
        "    ngram_lm : dict\n",
        "        The n-gram language model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The probability of the n-gram.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    ngram = tuple(ngram)\n",
        "    ngram_prob = ngram_lm[ngram] / sum(ngram_lm.values())\n",
        "    return ngram_prob\n",
        "\n",
        "ngram_prob((\"i\", \"do\", \"not\"), ngram_LM(3, [\"I do not like green eggs and ham\", \"I do not like them Sam I am\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpWcD0_RCBN5"
      },
      "source": [
        "## 3. Perplexity\n",
        "\n",
        "$$ PP(W) = P(w_1, w_2, \\cdots, w_n)^{-\\frac{1}{n}} = \\sqrt[n]{\\frac{1}{P(w_1, w_2, \\cdots, w_n)}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgt4Yw7YCBN5",
        "outputId": "a10770cd-121a-4b25-8301-305b6fce96b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12.337686603263526"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#perplexity\n",
        "\n",
        "def perplexity(sentence, n, ngram_lm):\n",
        "    \"\"\"\n",
        "    Calculates the perplexity of a sentence.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sentence : str\n",
        "        The sentence.\n",
        "    n : int\n",
        "        The length of the n-grams.\n",
        "    ngram_lm : dict\n",
        "        The n-gram language model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The perplexity of the sentence.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    ngrams = sentence_to_ngrams(sentence, n)\n",
        "    perplexity = 1\n",
        "    for ngram in ngrams:\n",
        "        perplexity *= 1/ngram_prob(ngram, ngram_lm)\n",
        "    perplexity = perplexity**(1/len(ngrams))\n",
        "    return perplexity\n",
        "\n",
        "perplexity(\"I do not like green eggs and ham\", 3, ngram_LM(3, [\"I do not like green eggs and ham\", \"I do not like them Sam I am\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INQlhwXNCBN6"
      },
      "source": [
        "## 4. Generalization\n",
        "\n",
        "!What is the problem of N-gram Language Model?\n",
        "\n",
        "[ANSWER] :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvWn222PCBN7"
      },
      "source": [
        "Laplace Smoothing\n",
        "\n",
        "$$ P_{Laplace}(w_i | w_{i-1}) = \\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V} $$"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cpwan",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}