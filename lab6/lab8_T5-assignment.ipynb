{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8 : T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@copyright: \n",
    "    (c) 2023. iKnow Lab. Ajou Univ., All rights reserved.\n",
    "\n",
    "M.S. Student: Wansik-Jo (jws5327@ajou.ac.kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For assignment\n",
    "\n",
    "- Python code의 주석 처리되어있는 부분을 구현하면 됩니다.\n",
    "- MD 형식의 Cell의 [BLANK] 부분을 채우면 됩니다.\n",
    "- MD 형식의 Cell의 [ANSWER] 부분 이후에 답을 작성하면 됩니다.\n",
    "- 조교에게 퀴즈의 답과 함께 코드 실행 결과를 보여준 뒤, BB에 제출 후 가시면 됩니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 목차\n",
    "\n",
    "1. T5 model\n",
    "2. T5 model for summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. T5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[T5](https://arxiv.org/abs/1910.10683)는 Text-to-Text Transfer Transformer의 약자로, NLP task를 하나의 text-to-text 문제로 통일하여 학습하는 모델이다.\n",
    "\n",
    "T5는 11가지의 NLP task를 하나의 모델로 학습시킨다.\n",
    "- Translation \n",
    "- Question Answering \n",
    "- Classification \n",
    "- Summarization \n",
    "- Paraphrasing \n",
    "- etc. \n",
    "\n",
    "![T5](./figure/t5.png)\n",
    "\n",
    "T5는 BERT와 같이 Transformer Encoder를 사용하며, BERT와 다르게 Decoder를 사용한다.\n",
    "즉, Seq2Seq 모델과 같이 Encoder-Decoder 구조를 가진다. 따라서, output으로 sequence를 생성할 수 있다.\n",
    "\n",
    "본 실습에서는 T5를 이용하여 Seq2Seq task인 Summarization task를 수행해본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 model의 tokenizing을 위해, [TorchText](https://pytorch.org/text/stable/transforms.html#sentencepiecetokenizer)를 사용한다.\n",
    "\n",
    "TorchText는 PyTorch에서 제공하는 NLP library로, 다양한 NLP task를 수행할 수 있는 기능을 제공한다.\n",
    "\n",
    "TorchText의 [SentencePieceTokenizer](https://github.com/google/sentencepiece)를 이용하여, T5 model의 input과 output을 tokenizing한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.models import T5Transform\n",
    "\n",
    "padding_idx = 0\n",
    "eos_idx = 1\n",
    "max_seq_len = 512\n",
    "t5_sp_model_path = \"https://download.pytorch.org/models/text/t5_tokenizer_base.model\"\n",
    "\n",
    "transform = T5Transform(\n",
    "    sp_model_path=t5_sp_model_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    eos_idx=eos_idx,\n",
    "    padding_idx=padding_idx,\n",
    ")\n",
    "\n",
    "# example\n",
    "print(transform([\"Hi this is I know lab NLP course!\", \"1 represent <EOS> token, and 0 represent <PAD> token\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. T5 model for summarization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset은 [CNN/Daily Mail](https://pytorch.org/text/stable/datasets.html.)으로, CNN과 Daily Mail의 기사를 학습 데이터로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import CNNDM\n",
    "\n",
    "cnndm_batch_size = 5\n",
    "cnndm_datapipe = CNNDM(split=\"test\")\n",
    "task = \"summarize\"\n",
    "\n",
    "def apply_prefix(task, x):\n",
    "    return f\"{task}: \" + x[0], x[1]\n",
    "\n",
    "cnndm_datapipe = cnndm_datapipe.map(partial(apply_prefix, task))\n",
    "cnndm_datapipe = cnndm_datapipe.batch(cnndm_batch_size)\n",
    "cnndm_datapipe = cnndm_datapipe.rows2columnar([\"article\", \"abstract\"])\n",
    "cnndm_dataloader = DataLoader(cnndm_datapipe, shuffle=True, batch_size=None)\n",
    "\n",
    "# example\n",
    "next(iter(cnndm_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.models import T5_BASE_GENERATION\n",
    "from torchtext.prototype.generate import GenerationUtils\n",
    "\n",
    "t5_base = T5_BASE_GENERATION\n",
    "transform = t5_base.transform()\n",
    "model = t5_base.get_model()\n",
    "model.eval()\n",
    "\n",
    "sequence_generator = GenerationUtils(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(cnndm_dataloader))\n",
    "input_text = batch[\"article\"]\n",
    "target = batch[\"abstract\"]\n",
    "beam_size = 1\n",
    "\n",
    "model_input = transform(input_text)\n",
    "model_output = sequence_generator.generate(model_input, eos_idx=eos_idx, num_beams=beam_size)\n",
    "output_text = transform.decode(model_output.tolist())\n",
    "\n",
    "for i in range(cnndm_batch_size):\n",
    "    print(f\"Example {i+1}:\\n\")\n",
    "    print(f\"prediction: {output_text[i]}\\n\")\n",
    "    print(f\"target: {target[i]}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpwan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
