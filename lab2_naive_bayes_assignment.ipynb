{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dh610/ai-intensive2/blob/main/lab2_naive_bayes_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/ai-intensive2\n",
        "\n",
        "!git pull"
      ],
      "metadata": {
        "id": "3BGwWdMOwOrF",
        "outputId": "e711fecd-0777-49c2-cddf-36ce54f3f8c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/ai-intensive2\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 18 (delta 11), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (18/18), 13.40 KiB | 24.00 KiB/s, done.\n",
            "From https://github.com/dh610/ai-intensive2\n",
            "   5f110da..76d1ed6  main       -> origin/main\n",
            "Updating 5f110da..76d1ed6\n",
            "Fast-forward\n",
            " lab2_LM_assignment.ipynb          |  852 \u001b[32m++++++++++++++++++++\u001b[m\u001b[31m---------------\u001b[m\n",
            " lab2_naive_bayes_assignment.ipynb | 1389 \u001b[32m+++++++++++++++++++++++++++++++++++\u001b[m\u001b[31m----------------------\u001b[m\n",
            " 2 files changed, 1339 insertions(+), 902 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI9LGEoewMOs"
      },
      "source": [
        "# Lab 2 : Naive Bayes Classifier for Document Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMgoC3JgwMOw"
      },
      "source": [
        "@copyright:\n",
        "    (c) 2023. iKnow Lab. Ajou Univ., All rights reserved.\n",
        "\n",
        "M.S. Student: Wansik-Jo (jws5327@ajou.ac.kr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1p2SbH5wMOx"
      },
      "source": [
        "# For assignment\n",
        "\n",
        "- Python code의 주석 처리되어있는 부분을 구현하면 됩니다.\n",
        "- MD 형식의 Cell의 [BLANK] 부분을 채우면 됩니다.\n",
        "- MD 형식의 Cell의 [ANSWER] 부분 이후에 답을 작성하면 됩니다.\n",
        "- 조교에게 퀴즈의 답과 함께 코드 실행 결과를 보여준 뒤, BB에 제출 후 가시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBj-V84-wMOy"
      },
      "source": [
        "\n",
        "## 목차\n",
        "1. Document Classification task 이해하기\n",
        "    - Dataset 살펴보기\n",
        "    - Task 이해하기\n",
        "2. Data Preprocessing\n",
        "    - Dataloader 구현하기\n",
        "    - Data 확인하기\n",
        "    - Data preprocessing 구현하기\n",
        "3. Naive Bayes Classifier 구현하기\n",
        "    - 수식 이해하기\n",
        "    - Vocabulary 구축하기\n",
        "    - Prior 계산하기\n",
        "    - Likelihood 계산하기\n",
        "    - Posterior 계산하기\n",
        "    - Prediction 구하기\n",
        "    - Model 학습하기\n",
        "    - Model 성능 평가하기\n",
        "4. Naive Bayes Classifier의 여러 변형 모델 응용하기\n",
        "    - 과제\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akBBeFV-wMOz"
      },
      "source": [
        "## 1. Document Classification task 이해하기\n",
        "- Dataset: NewsGroup Documents datasets\n",
        "- Task: Document Classification\n",
        "- Input: Document\n",
        "- Output: Document Category\n",
        "- Model: Naive Bayes Classifier\n",
        "- Evaluation: Confusion Matrix, Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDEE_yS-wMO0"
      },
      "source": [
        "### Document Classification dataset\n",
        "\n",
        "- URL : https://www.kaggle.com/datasets/jensenbaxter/10dataset-text-document-classification\n",
        "\n",
        "- Kaggle 10 group dataset\n",
        "- 각 Document는, 10개의 category 중 하나에 속함\n",
        "- Categories : business, entertainment, food, graphics, historical, medical, politics, space, sport, technologie\n",
        "- URL 또는 강의노트에서 제공되는 data를 받아서 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lvav1xX4wMO1",
        "outputId": "64b32772-a420-40c5-8ef1-ea9774df8ba5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800 200\n"
          ]
        }
      ],
      "source": [
        "#dataset load\n",
        "categories = {'business', 'entertainment', 'food', 'graphics', 'historical', 'medical', 'politics', 'space', 'sport', 'technologie'}\n",
        "\n",
        "import os\n",
        "\n",
        "dataset = []\n",
        "for category in categories:\n",
        "    for filename in os.listdir('data/' + category):\n",
        "        with open('data/' + category + '/' + filename, 'r') as f:\n",
        "            instance = {}\n",
        "            instance['text'] = f.read()\n",
        "            instance['category'] = category\n",
        "            dataset.append(instance)\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(dataset)\n",
        "train_data = dataset[:int(len(dataset) * 0.8)]\n",
        "test_data = dataset[int(len(dataset) * 0.8):]\n",
        "\n",
        "print(len(train_data), len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFk08Yb6wMO6",
        "outputId": "5dc89b6e-2934-4850-d7de-38dd18b259f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pavey focuses on indoor success\n",
            "\n",
            "Jo Pavey will miss January's View From Great Edinburgh International Cross Country to focus on preparing for the European Indoor Championships in March.\n",
            "\n",
            "The 31-year-old was third behind Hayley Yelling and Justyna Bak in last week's European Cross Country Championships but she prefers to race on the track. \"It was great winning bronze but I'm wary of injuries and must concentrate on the indoor season,\" she said. \"Because of previous injuries I don't even run up hills in training.\" Pavey, who came fifth in the 5,000m at the Athens Olympics, helped the British cross country team win the team silver medal in Heringsdorf last week. She is likely to start her 3,000m season with a race in either Boston or Stuttgart at the end of January.\n",
            "\n",
            "sport\n",
            "Counter({'historical': 87, 'technologie': 83, 'food': 83, 'space': 83, 'graphics': 83, 'medical': 78, 'entertainment': 77, 'politics': 76, 'sport': 75, 'business': 75})\n"
          ]
        }
      ],
      "source": [
        "#data 확인\n",
        "print(train_data[0]['text'])\n",
        "print(train_data[0]['category'])\n",
        "\n",
        "#label 비율 확인\n",
        "from collections import Counter\n",
        "print(Counter([instance['category'] for instance in train_data]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt7RnNa3wMO7"
      },
      "source": [
        "## 2. Data Preprocessing 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EDeLJb4wMO7"
      },
      "source": [
        "## Document raw data preprocessing\n",
        "\n",
        "- 학습한 여러 기법을 활용하여 데이터 전처리를 진행\n",
        "- Tokenizing, Stemming, Lemmatizing, Stopword 제거, Punctuation 제거, Case 변환 등 다양한 NLP pre processing 방법을 적용\n",
        "- News data에 맞는 e-mail, url, phone number, number, date, time 등의 특수문자 제거 (Regex)\n",
        "\n",
        "- !nltk library 만 사용 가능 (import nltk)\n",
        "- !단 e-mail, url, phone number, number, date, time 등에 관한 처리는 regex를 사용하여 직접 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK9mb9aQwMO8",
        "outputId": "64103068-519a-4067-8593-a224a68e0404",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "#You can use pre-defined tools in nltk, like Tokenizer, Stopword list, etc.\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemm = nltk.stem.WordNetLemmatizer()\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "def clean_text(instance):\n",
        "    #You should implement any pre-processing if you need.\n",
        "    text = instance['text'].lower()\n",
        "\n",
        "    #Or like here, you can use regular expression to remove any unwanted characters.\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    \"\"\"\n",
        "    From here, you should implement any pre-process method like tokenizer, stopword list, stemming, lemmatization, etc.\n",
        "    Or the regular expression above can be considered.\n",
        "\n",
        "    text =\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #print(text)\n",
        "\n",
        "    text = nltk.word_tokenize(text)\n",
        "    #print(text)\n",
        "    text = [word for word in text if word not in stopwords]\n",
        "    #print(text)\n",
        "    text = [lemm.lemmatize(word) for word in text]\n",
        "    #print(text)\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "    #print(text)\n",
        "    text = [word for word in text if word not in string.punctuation]\n",
        "    #print(text)\n",
        "\n",
        "    instance['tokens'] = text\n",
        "\n",
        "for instance in train_data:\n",
        "    clean_text(instance)\n",
        "\n",
        "for instance in test_data:\n",
        "    clean_text(instance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCanrt5_wMO8",
        "outputId": "cf3eadf2-e796-477e-d02f-5ae246799575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pavey focuses on indoor success\n",
            "\n",
            "Jo Pavey will miss January's View From Great Edinburgh International Cross Country to focus on preparing for the European Indoor Championships in March.\n",
            "\n",
            "The 31-year-old was third behind Hayley Yelling and Justyna Bak in last week's European Cross Country Championships but she prefers to race on the track. \"It was great winning bronze but I'm wary of injuries and must concentrate on the indoor season,\" she said. \"Because of previous injuries I don't even run up hills in training.\" Pavey, who came fifth in the 5,000m at the Athens Olympics, helped the British cross country team win the team silver medal in Heringsdorf last week. She is likely to start her 3,000m season with a race in either Boston or Stuttgart at the end of January.\n",
            "\n",
            "['pavey', 'focu', 'indoor', 'success', 'jo', 'pavey', 'miss', 'januari', 'view', 'great', 'edinburgh', 'intern', 'cross', 'countri', 'focu', 'prepar', 'european', 'indoor', 'championship', 'march', 'yearold', 'third', 'behind', 'hayley', 'yell', 'justyna', 'bak', 'last', 'week', 'european', 'cross', 'countri', 'championship', 'prefer', 'race', 'track', 'great', 'win', 'bronz', 'im', 'wari', 'injuri', 'must', 'concentr', 'indoor', 'season', 'said', 'previou', 'injuri', 'dont', 'even', 'run', 'hill', 'train', 'pavey', 'came', 'fifth', 'athen', 'olymp', 'help', 'british', 'cross', 'countri', 'team', 'win', 'team', 'silver', 'medal', 'heringsdorf', 'last', 'week', 'like', 'start', 'season', 'race', 'either', 'boston', 'stuttgart', 'end', 'januari']\n"
          ]
        }
      ],
      "source": [
        "#check\n",
        "print(train_data[0]['text'])\n",
        "print(train_data[0]['tokens'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNXpzSgSwMO9"
      },
      "source": [
        "## 3. Naive Bayes Classifier 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hnDmlvowMO9"
      },
      "source": [
        "### Naive Bayes to Documents and Categories\n",
        "\n",
        "document $d$ 와 category $c$ 에 대하여, posterior probability는 다음과 같이 구할 수 있다.\n",
        "\n",
        "$$ P(c|d) = \\frac{P(d|c)P(c)}{P(d)} $$\n",
        "\n",
        "이때 $P(c)$ 를 category $c$ 의[Prior],\n",
        "\n",
        "$P(d|c)$ 를 category $c$ 에서 document $d$ 의 [Likelihood],\n",
        "\n",
        "$P(d)$ 를 document $d$ 의 [Prior] 라고 한다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g8tyBVowMO9"
      },
      "source": [
        "### Naive Bayes Classifier\n",
        "\n",
        "이 때, MAP(most probable category) $c_{MAP}$ for a document $d$ 는 다음과 같이 구할 수 있다.\n",
        "\n",
        "$$ C_{MAP} = \\underset{c \\in C}{\\operatorname{argmax}} P(c|d) $$\n",
        "\n",
        "그러나, $P(c|d)$ 를 직접 계산하기는 어렵다. ($P(d)$ 를 계산하기 어렵기 때문에)\n",
        "\n",
        "따라서, 여기서 Bayes' theorem 을 사용하여 $P(c|d)$ 를 다음과 같이 바꿀 수 있다.\n",
        "\n",
        "$$ P(c|d) = \\frac{P(d|c)P(c)}{P(d)} $$\n",
        "\n",
        "</br>\n",
        "\n",
        "이때, $P(d)$ 는 모든 category $c$ 에 대하여 동일하므로, $P(d)$ 는 상수이다. (계산할 필요가 없다.)\n",
        "\n",
        "따라서,\n",
        "\n",
        "$$ C_{MAP} = \\underset{c \\in C}{\\operatorname{argmax}} P(d|c)P(c) $$\n",
        "\n",
        "이때, $P(d|c)$ 를 [Likelihood], $P(c)$ 를 [Prior] 라고 한다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUyP68RCwMO-"
      },
      "source": [
        "### Multinomial Naive Bayes Classifier\n",
        "\n",
        "likelihood $P(d|c)$ 는 다음과 같이 계산할 수 있다.\n",
        "\n",
        "$$ P(d|c) = P(w_1, w_2, \\cdots, w_n|c) = \\prod_{i=1}^{n} P(w_i|c) $$\n",
        "\n",
        "이때 $w_i$ 는 $i$-th 번째 word in document $d$ 이다.\n",
        "\n",
        "따라서, 최종 식은 다음과 같다.\n",
        "\n",
        "$$ C_{NB} = \\underset{c \\in C}{\\operatorname{argmax}} P(c) \\prod_{i=1}^{n} P(w_i|c) $$\n",
        "\n",
        "! 여기서 발생할 수 있는 문제점은?\n",
        "\n",
        "[ANSWER] : 값이 너무 작아져서 컴퓨터가 값을 표현 못하는 경우가 발생(값 손실)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X03TW56VwMO-"
      },
      "source": [
        "### Log Likelihood\n",
        "\n",
        "문제는 likelihood $P(d|c)$ 가 매우 작은 값이 될 수 있다는 것이다.\n",
        "\n",
        "따라서, likelihood 대신 log likelihood 를 사용하면 다음과 같다.\n",
        "\n",
        "$$ C_{NB} = \\underset{c \\in C}{\\operatorname{argmax}} P(c) \\sum_{i=1}^{n} \\log P(w_i|c) $$\n",
        "\n",
        "! 여기서 발생할 수 있는 문제점은?\n",
        "\n",
        "[ANSWER] : word가 한번도 등장하지 않을 경우 전체 값이 0이 돼버린다.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW8atjaSwMO-"
      },
      "source": [
        "### Laplace smoothing\n",
        "\n",
        "문제는 likelihood $P(d|c)$ 가 0이 될 수 있다는 것이다.\n",
        "\n",
        "따라서 zero0-probability 를 방지하기 위하여 Laplace smoothing 을 사용한다.\n",
        "\n",
        "$$ P(w_i|c) = \\frac{count(w_i, c) + \\alpha}{count(c) + \\alpha \\times |V|} $$\n",
        "\n",
        "이때, $count(w_i, c)$ 는 category $c$ 에서 $w_i$ 의 개수이고\n",
        "\n",
        "$count(c)$ 는 category $c$ 에서 word 의 총 개수,\n",
        "\n",
        "$|V|$ 는 vocabulary 의 길이.\n",
        "\n",
        "$\\alpha$ 는 smoothing parameter 이다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udjbfQOfwMO-",
        "outputId": "da27f921-e30d-4457-af8c-a56573fe3516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'medical', 'graphics', 'politics', 'space', 'historical', 'technologie', 'entertainment', 'sport', 'food', 'business'}\n",
            "152478\n"
          ]
        }
      ],
      "source": [
        "#make vocabulary\n",
        "vocabulary = set()\n",
        "for instance in train_data:\n",
        "    vocabulary.update(instance['tokens'])\n",
        "\n",
        "print(categories)\n",
        "print(len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iK_tjhhwMO_"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "def train_classifier(data, categories, vocabulary):\n",
        "    prior = {category: 0 for category in categories}\n",
        "    likelihood = defaultdict(lambda: defaultdict(int))\n",
        "    category_counts = {category: 0 for category in categories} # Initialize category_counts to track total tokens per category\n",
        "\n",
        "    #You should implement the training process of Naive Bayes classifier.\n",
        "    alpha = 1\n",
        "\n",
        "    for instance in data:\n",
        "        category = instance['category']\n",
        "        for token in instance['tokens']:\n",
        "            \"\"\"\n",
        "            Here, you should calculate the prior and likelihood.\n",
        "\n",
        "            \"\"\"\n",
        "            likelihood[category][token] += 1\n",
        "            category_counts[category] += 1\n",
        "        prior[category] += 1\n",
        "\n",
        "    total_instances = len(data)\n",
        "    for category in categories:\n",
        "        \"\"\"\n",
        "        Here, calculate the prior and likelihood, for each category.\n",
        "        Like we have learned above, you should use Log-likelihood, and Laplace smoothing.\n",
        "\n",
        "        prior[category] =\n",
        "        for word in vocabulary:\n",
        "            likelihood[category][word] =\n",
        "\n",
        "        \"\"\"\n",
        "        prior[category] = math.log(prior[category]) - math.log(total_instances)\n",
        "        for word in vocabulary:\n",
        "            cnt_word = likelihood[category][word] + alpha\n",
        "            cnt_all = category_counts[category] + alpha * len(vocabulary)\n",
        "            likelihood[category][word] = math.log(cnt_word) - math.log(cnt_all)\n",
        "\n",
        "\n",
        "    return prior, likelihood, category_counts\n",
        "\n",
        "prior, likelihood, category_counts = train_classifier(train_data, categories, vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4z5f9kJwMO_",
        "outputId": "3cd10751-465d-4e58-f4ac-b92d3340c86a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business\n"
          ]
        }
      ],
      "source": [
        "def predict(instance, prior, likelihood, categories, category_counts, vocabulary):\n",
        "    score = {}\n",
        "\n",
        "    #You should implement the prediction process of Naive Bayes classifier.\n",
        "\n",
        "    for category in categories:\n",
        "        \"\"\"\n",
        "        Here, you should calculate the score of each category.\n",
        "        score[category] = prior[category]\n",
        "        for token in instance['tokens']:\n",
        "            score[catetory] +=\n",
        "            You should consider the case when the token is unseen here.\n",
        "\n",
        "        \"\"\"\n",
        "        score[category] = prior[category]\n",
        "        for token in instance['tokens']:\n",
        "            if token in vocabulary:\n",
        "                score[category] += likelihood[category][token]\n",
        "\n",
        "    return max(score, key=score.get)\n",
        "\n",
        "print(predict(test_data[0], prior, likelihood, categories, category_counts, vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsyHtRy7wMPA",
        "outputId": "4a6f2130-e3e5-4ae7-f212-9a199272c09d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.95 {'medical': 0.9545454545454546, 'graphics': 1.0, 'politics': 0.9230769230769231, 'space': 0.8888888888888888, 'historical': 1.0, 'technologie': 0.7727272727272727, 'entertainment': 1.0, 'sport': 1.0, 'food': 1.0, 'business': 1.0} {'medical': 0.9545454545454546, 'graphics': 0.9411764705882353, 'politics': 1.0, 'space': 0.9411764705882353, 'historical': 1.0, 'technologie': 1.0, 'entertainment': 0.8260869565217391, 'sport': 1.0, 'food': 0.9411764705882353, 'business': 0.92}\n"
          ]
        }
      ],
      "source": [
        "def evaluate(data, prior, likelihood, categories, category_counts, vocabulary):\n",
        "    metrics = {\n",
        "        'TP': {category: 0 for category in categories},\n",
        "        'TN': {category: 0 for category in categories},\n",
        "        'FP': {category: 0 for category in categories},\n",
        "        'FN': {category: 0 for category in categories},\n",
        "    }\n",
        "\n",
        "    #You should implement confusion matrix here. Fill in if statement below.\n",
        "    for instance in data:\n",
        "        true_category = instance['category']\n",
        "        predicted_category = predict(instance, prior, likelihood, categories, category_counts, vocabulary)\n",
        "\n",
        "        for category in categories:\n",
        "            if true_category == category and predicted_category == category:\n",
        "                metrics['TP'][category] += 1\n",
        "            if true_category != category and predicted_category != category:\n",
        "                metrics['TN'][category] += 1\n",
        "            if true_category != category and predicted_category == category:\n",
        "                metrics['FP'][category] += 1\n",
        "            if true_category == category and predicted_category != category:\n",
        "                metrics['FN'][category] += 1\n",
        "\n",
        "    precision = {category: metrics['TP'][category] / (metrics['TP'][category] + metrics['FP'][category]) for category in categories}\n",
        "    recall = {category: metrics['TP'][category] / (metrics['TP'][category] + metrics['FN'][category]) for category in categories}\n",
        "    accuracy = sum(metrics['TP'].values()) / len(data)\n",
        "\n",
        "    return accuracy, precision, recall\n",
        "\n",
        "accuracy, precision, recall = evaluate(test_data, prior, likelihood, categories, category_counts, vocabulary)\n",
        "print(accuracy, precision, recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87XRUf7dwMPB"
      },
      "source": [
        "## 4. Naive Bayes Classifier의 여러 변형 모델 응용하기\n",
        "\n",
        "- N-gram을 활용한 Naive Bayes Classifier (과제)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th6JAh_4wMPB",
        "outputId": "49be67f1-a6ac-4af7-8e3a-16d0e2c505a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "#You can use pre-defined tools in nltk, like Tokenizer, Stopword list, etc.\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemm = nltk.stem.WordNetLemmatizer()\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "def clean_text_with_ngrams(instance, n):\n",
        "    #You should implement any pre-processing if you need.\n",
        "    text = instance['text'].lower()\n",
        "\n",
        "    #Or like here, you can use regular expression to remove any unwanted characters.\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    \"\"\"\n",
        "    From here, you should implement any pre-process method like tokenizer, stopword list, stemming, lemmatization, etc.\n",
        "    Or the regular expression above can be considered.\n",
        "\n",
        "    text =\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    text = nltk.word_tokenize(text)\n",
        "    text = [word for word in text if word not in stopwords]\n",
        "    text = [lemm.lemmatize(word) for word in text]\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "    text = [word for word in text if word not in string.punctuation]\n",
        "\n",
        "    ngram_text = [tuple(text[i : i + n]) for i in range(len(text) - n)]\n",
        "\n",
        "    instance['tokens'] = ngram_text\n",
        "\n",
        "for instance in train_data:\n",
        "    clean_text_with_ngrams(instance, 2)\n",
        "\n",
        "for instance in test_data:\n",
        "    clean_text_with_ngrams(instance, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check\n",
        "print(train_data[0]['text'])\n",
        "print(train_data[0]['tokens'])"
      ],
      "metadata": {
        "id": "R6qFr2ndPk7i",
        "outputId": "b215945f-3b54-4aac-bd35-87f423396d2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pavey focuses on indoor success\n",
            "\n",
            "Jo Pavey will miss January's View From Great Edinburgh International Cross Country to focus on preparing for the European Indoor Championships in March.\n",
            "\n",
            "The 31-year-old was third behind Hayley Yelling and Justyna Bak in last week's European Cross Country Championships but she prefers to race on the track. \"It was great winning bronze but I'm wary of injuries and must concentrate on the indoor season,\" she said. \"Because of previous injuries I don't even run up hills in training.\" Pavey, who came fifth in the 5,000m at the Athens Olympics, helped the British cross country team win the team silver medal in Heringsdorf last week. She is likely to start her 3,000m season with a race in either Boston or Stuttgart at the end of January.\n",
            "\n",
            "[('pavey', 'focu'), ('focu', 'indoor'), ('indoor', 'success'), ('success', 'jo'), ('jo', 'pavey'), ('pavey', 'miss'), ('miss', 'januari'), ('januari', 'view'), ('view', 'great'), ('great', 'edinburgh'), ('edinburgh', 'intern'), ('intern', 'cross'), ('cross', 'countri'), ('countri', 'focu'), ('focu', 'prepar'), ('prepar', 'european'), ('european', 'indoor'), ('indoor', 'championship'), ('championship', 'march'), ('march', 'yearold'), ('yearold', 'third'), ('third', 'behind'), ('behind', 'hayley'), ('hayley', 'yell'), ('yell', 'justyna'), ('justyna', 'bak'), ('bak', 'last'), ('last', 'week'), ('week', 'european'), ('european', 'cross'), ('cross', 'countri'), ('countri', 'championship'), ('championship', 'prefer'), ('prefer', 'race'), ('race', 'track'), ('track', 'great'), ('great', 'win'), ('win', 'bronz'), ('bronz', 'im'), ('im', 'wari'), ('wari', 'injuri'), ('injuri', 'must'), ('must', 'concentr'), ('concentr', 'indoor'), ('indoor', 'season'), ('season', 'said'), ('said', 'previou'), ('previou', 'injuri'), ('injuri', 'dont'), ('dont', 'even'), ('even', 'run'), ('run', 'hill'), ('hill', 'train'), ('train', 'pavey'), ('pavey', 'came'), ('came', 'fifth'), ('fifth', 'athen'), ('athen', 'olymp'), ('olymp', 'help'), ('help', 'british'), ('british', 'cross'), ('cross', 'countri'), ('countri', 'team'), ('team', 'win'), ('win', 'team'), ('team', 'silver'), ('silver', 'medal'), ('medal', 'heringsdorf'), ('heringsdorf', 'last'), ('last', 'week'), ('week', 'like'), ('like', 'start'), ('start', 'season'), ('season', 'race'), ('race', 'either'), ('either', 'boston'), ('boston', 'stuttgart'), ('stuttgart', 'end')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make vocabulary\n",
        "vocabulary = set()\n",
        "for instance in train_data:\n",
        "    vocabulary.update(instance['tokens'])\n",
        "\n",
        "print(categories)\n",
        "print(len(vocabulary))"
      ],
      "metadata": {
        "id": "QdGWeD9vTD7D",
        "outputId": "a094a457-0335-44a1-f64d-0e9721f86d4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'medical', 'graphics', 'politics', 'space', 'historical', 'technologie', 'entertainment', 'sport', 'food', 'business'}\n",
            "152478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "prior, likelihood, category_counts = train_classifier(train_data, categories, vocabulary)\n",
        "print(predict(test_data[0], prior, likelihood, categories, category_counts, vocabulary))\n",
        "accuracy, precision, recall = evaluate(test_data, prior, likelihood, categories, category_counts, vocabulary)\n",
        "print(accuracy, precision, recall)"
      ],
      "metadata": {
        "id": "hWujNSQ4QOtA",
        "outputId": "abb71c33-75cd-44a1-e7de-f2664c73888a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business\n",
            "0.92 {'medical': 1.0, 'graphics': 1.0, 'politics': 0.8888888888888888, 'space': 0.875, 'historical': 0.8666666666666667, 'technologie': 0.7727272727272727, 'entertainment': 0.95, 'sport': 0.8928571428571429, 'food': 1.0, 'business': 1.0} {'medical': 0.8636363636363636, 'graphics': 0.7647058823529411, 'politics': 1.0, 'space': 0.8235294117647058, 'historical': 1.0, 'technologie': 1.0, 'entertainment': 0.8260869565217391, 'sport': 1.0, 'food': 1.0, 'business': 0.92}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cpwan",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}